output_dir
2023-09-14 23:39:17 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-09-14 23:39:20 | INFO | fairseq_cli.train | {'task': {'_name': 'fs_eval', 'data': '-', 'seed': 4, 'eval_data': 'subj', 'test_split': 'test', 'required_batch_size_multiple': 1, 'gpt2_encoder_json': 'base_dir/gpt_icl/encoder.json', 'gpt2_vocab_bpe': 'base_dir/gpt_icl/vocab.bpe', 'gpt_dict': 'base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', 'ana_attn': 1, 'ana_rlt_dir': 'base_dir/ana_rlt/en_dense_lm_1_3b/subj', 'ana_setting': 'ft', 'optim_group': 'attn_kv', 'tokens_per_sample': 2048, 'max_target_positions': None, 'k': 32, 'temp_index': 0, 'permut_index': 0}, 'model': {'_name': 'gptmodel_large', 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 2048, 'decoder_output_dim': 2048, 'decoder_input_dim': 2048, 'decoder_ffn_embed_dim': 8192, 'decoder_layers': 24, 'decoder_attention_heads': 32, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 2048, 'max_target_positions': None, 'tpu': False, 'gpt_model_path': 'base_dir/gpt_icl/en_dense_lm_1_3b/model.pt', 'use_linearization': '0', 'sum_extra_jvp_result': True}, 'criterion': {'_name': 'fs_ft', 'is_generation': False, 'beam': 3}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.003]}, 'common': {'no_progress_bar': False, 'log_interval': 1, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'distributed_world_size': 1, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1000000, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 1000000, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'max_epoch': 1, 'max_update': 1000000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.003], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'save_dir': 'base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'beam': 3, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'ema': {'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'tokenizer': None, 'bpe': None, 'optimizer': Namespace(no_progress_bar=False, log_interval=1, log_format='simple', log_file=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=4, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', tokenizer=None, bpe=None, optimizer='sgd', lr_scheduler='fixed', criterion='fs_ft', task='fs_eval', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=1, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1000000, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=True, max_tokens_valid=None, batch_size_valid='1', max_valid_steps=None, curriculum=1000000, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=True, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='gptmodel_large', max_epoch=1, max_update=1000000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.003], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=1000000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='-', eval_data='subj', test_split='test', gpt2_encoder_json='base_dir/gpt_icl/encoder.json', gpt2_vocab_bpe='base_dir/gpt_icl/vocab.bpe', gpt_dict='base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', ana_attn=1, ana_rlt_dir='base_dir/ana_rlt/en_dense_lm_1_3b/subj', ana_setting='ft', optim_group='attn_kv', tokens_per_sample=2048, max_target_positions=None, k=32, temp_index=0, permut_index=0, momentum=0.0, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, is_generation=False, beam=3, checkpoint_activations=True, gpt_model_path='base_dir/gpt_icl/en_dense_lm_1_3b/model.pt', use_linearization='0', no_seed_provided=False, decoder_layers=24, decoder_embed_dim=2048, decoder_attention_heads=32, decoder_learned_pos=False, offload_activations=False, decoder_input_dim=2048, decoder_output_dim=2048, decoder_ffn_embed_dim=8192, dropout=0.0, attention_dropout=0.0, activation_fn='gelu', share_decoder_input_output_embed=True, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, decoder_layerdrop=0, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, base_layers=0, base_sublayers=1, base_shuffle=False, add_bos_token=False, no_token_positional_embeddings=False, character_embeddings=False, decoder_normalize_before=True, no_decoder_final_norm=False, adaptive_input=False, adaptive_input_factor=4, adaptive_input_cutoff=None, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=False, layernorm_embedding=False, scale_fc=False, scale_attn=False, scale_heads=False, scale_resids=False, _name='sgd')}
2023-09-14 23:39:20 | INFO | struprompting.tasks.fs_eval | dictionary: 51200 types
2023-09-14 23:39:22 | WARNING | datasets.builder | Using custom data configuration SetFit--subj-693a635c625bebac
2023-09-14 23:39:22 | WARNING | datasets.builder | Reusing dataset json (/a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 306.99it/s]
2023-09-14 23:39:22 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-41679576bb6cfc7c.arrow
2023-09-14 23:39:22 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-6b58bb8b23415efd.arrow
/vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/struprompting/tasks/fewshot_task.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  return np.array(src_tokens), np.array(gpt_loss_mask), np.array(labels), max_len
2023-09-14 23:39:48 | WARNING | fairseq.models.fairseq_model | using 'args' is deprecated, please update your code to use dataclass config
2023-09-14 23:39:49 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): GPTDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 2048, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=2048, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=2048, bias=True)
        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=2048, out_features=51200, bias=False)
  )
)
2023-09-14 23:39:49 | INFO | fairseq_cli.train | task: FewshotEval
2023-09-14 23:39:49 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2023-09-14 23:39:49 | INFO | fairseq_cli.train | criterion: FewshotFTCriterion
2023-09-14 23:39:49 | INFO | fairseq_cli.train | num. shared model params: 1,313,460,224 (num. trained: 1,313,460,224)
2023-09-14 23:39:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-14 23:39:49 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-09-14 23:39:49 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-09-14 23:39:49 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 1
2023-09-14 23:39:49 | INFO | fairseq.trainer | Preparing to load checkpoint base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint_last.pt
2023-09-14 23:39:49 | INFO | fairseq.trainer | No existing checkpoint found base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint_last.pt
2023-09-14 23:39:49 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-14 23:39:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32
2023-09-14 23:39:49 | INFO | fairseq.trainer | begin training epoch 1
2023-09-14 23:39:49 | INFO | fairseq_cli.train | Start fine-tuning
------------ example 0 input str train is ['Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:', 'Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:']
------------ example 0 label str train is [' objective', ' subjective']
------------ example 1 input str train is ["Input: and if she 's lucky , she may find love along the way . Type:", "Input: and if she 's lucky , she may find love along the way . Type:"]
------------ example 1 label str train is [' objective', ' subjective']
------------ example 0 input str valid is ['Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:', 'Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:']
------------ example 0 label str valid is [' objective', ' subjective']
------------ example 1 input str valid is ['Input: if the story lacks bite , the performances are never less than affectionate . Type:', 'Input: if the story lacks bite , the performances are never less than affectionate . Type:']
------------ example 1 label str valid is [' objective', ' subjective']
NOTE: true K of baseline is 32, max valid len is 174
------------ example 0 input str train is ['Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:', 'Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:']
------------ example 0 label str train is [' objective', ' subjective']
------------ example 1 input str train is ["Input: and if she 's lucky , she may find love along the way . Type:", "Input: and if she 's lucky , she may find love along the way . Type:"]
------------ example 1 label str train is [' objective', ' subjective']
NOTE: true K of baseline is 32
| Loaded valid with 4000 samples
| Loaded train with 32 samples
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2566.25927734375
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 1 grad norm = 1834.1456298828125
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1018.3802490234375
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 3 grad norm = 860.0450439453125
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 4 grad norm = 645.5286865234375
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 5 grad norm = 488.0528869628906
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 6 grad norm = 303.4843444824219
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 7 grad norm = 217.4318389892578
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 8 grad norm = 158.96908569335938
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 9 grad norm = 114.14244842529297
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 10 grad norm = 98.54483032226562
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 11 grad norm = 84.85284423828125
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 12 grad norm = 85.17239379882812
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 13 grad norm = 75.35453033447266
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 14 grad norm = 73.82286071777344
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 15 grad norm = 77.37553405761719
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 16 grad norm = 79.4796142578125
2023-09-14 23:39:55 | INFO | fairseq.trainer | decoder layer 17 grad norm = 87.34432220458984
2023-09-14 23:39:56 | INFO | fairseq.trainer | decoder layer 18 grad norm = 80.2255859375
2023-09-14 23:39:56 | INFO | fairseq.trainer | decoder layer 19 grad norm = 72.35281372070312
2023-09-14 23:39:56 | INFO | fairseq.trainer | decoder layer 20 grad norm = 65.34772491455078
2023-09-14 23:39:56 | INFO | fairseq.trainer | decoder layer 21 grad norm = 68.17500305175781
2023-09-14 23:39:56 | INFO | fairseq.trainer | decoder layer 22 grad norm = 70.32609558105469
2023-09-14 23:39:56 | INFO | fairseq.trainer | decoder layer 23 grad norm = 112.26861572265625
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 0 total_norm 2566.25927734375
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 1 total_norm 1834.1456298828125
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 2 total_norm 1018.3802490234375
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 3 total_norm 860.0450439453125
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 4 total_norm 645.5286865234375
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 5 total_norm 488.0528869628906
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 6 total_norm 303.4843444824219
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 7 total_norm 217.4318389892578
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 8 total_norm 158.96908569335938
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 9 total_norm 114.14244842529297
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 10 total_norm 98.54483032226562
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 11 total_norm 84.85284423828125
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 12 total_norm 85.17239379882812
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 13 total_norm 75.35453033447266
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 14 total_norm 73.82286071777344
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 15 total_norm 77.37553405761719
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 16 total_norm 79.4796142578125
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 17 total_norm 87.34432220458984
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 18 total_norm 80.2255859375
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 19 total_norm 72.35281372070312
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 20 total_norm 65.34772491455078
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 21 total_norm 68.17500305175781
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 22 total_norm 70.32609558105469
2023-09-14 23:39:56 | INFO | fairseq.trainer | layer 23 total_norm 112.26861572265625
2023-09-14 23:39:57 | INFO | train_inner | epoch 001:      1 / 32 loss=490.268, sample_size=28, ntokens=29, wps=0, ups=0, wpb=29, bsz=1, num_updates=1, lr=0.003, gnorm=38.689, clip=100, train_wall=8, wall=8
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2461.744873046875
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2036.5206298828125
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1533.740234375
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1370.138427734375
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1051.9932861328125
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 5 grad norm = 741.134033203125
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 6 grad norm = 486.3654479980469
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 7 grad norm = 333.36932373046875
2023-09-14 23:40:00 | INFO | fairseq.trainer | decoder layer 8 grad norm = 239.92486572265625
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 9 grad norm = 175.4334716796875
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 10 grad norm = 138.83946228027344
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 11 grad norm = 120.63186645507812
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 12 grad norm = 118.7916259765625
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 13 grad norm = 106.49369049072266
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 14 grad norm = 102.89413452148438
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 15 grad norm = 108.7403335571289
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 16 grad norm = 109.6565170288086
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 17 grad norm = 123.78270721435547
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 18 grad norm = 108.72454071044922
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 19 grad norm = 96.98150634765625
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 20 grad norm = 81.2388916015625
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 21 grad norm = 94.01024627685547
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 22 grad norm = 94.42695617675781
2023-09-14 23:40:01 | INFO | fairseq.trainer | decoder layer 23 grad norm = 189.1002197265625
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 0 total_norm 2461.744873046875
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 1 total_norm 2036.5206298828125
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 2 total_norm 1533.740234375
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 3 total_norm 1370.138427734375
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 4 total_norm 1051.9932861328125
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 5 total_norm 741.134033203125
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 6 total_norm 486.3654479980469
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 7 total_norm 333.36932373046875
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 8 total_norm 239.92486572265625
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 9 total_norm 175.4334716796875
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 10 total_norm 138.83946228027344
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 11 total_norm 120.63186645507812
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 12 total_norm 118.7916259765625
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 13 total_norm 106.49369049072266
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 14 total_norm 102.89413452148438
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 15 total_norm 108.7403335571289
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 16 total_norm 109.6565170288086
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 17 total_norm 123.78270721435547
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 18 total_norm 108.72454071044922
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 19 total_norm 96.98150634765625
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 20 total_norm 81.2388916015625
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 21 total_norm 94.01024627685547
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 22 total_norm 94.42695617675781
2023-09-14 23:40:01 | INFO | fairseq.trainer | layer 23 total_norm 189.1002197265625
2023-09-14 23:40:01 | INFO | train_inner | epoch 001:      2 / 32 loss=430.626, sample_size=20, ntokens=21, wps=4.3, ups=0.2, wpb=21, bsz=1, num_updates=2, lr=0.003, gnorm=36.591, clip=100, train_wall=5, wall=13
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3630.761474609375
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2840.773681640625
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1970.5640869140625
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1710.61572265625
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1407.7044677734375
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1166.6112060546875
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 6 grad norm = 749.8980102539062
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 7 grad norm = 507.0394287109375
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 8 grad norm = 380.11761474609375
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 9 grad norm = 270.3086242675781
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 10 grad norm = 206.04443359375
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 11 grad norm = 169.83583068847656
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 12 grad norm = 181.17218017578125
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 13 grad norm = 151.05743408203125
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 14 grad norm = 145.79908752441406
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 15 grad norm = 152.59828186035156
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 16 grad norm = 146.611328125
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 17 grad norm = 163.74676513671875
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 18 grad norm = 144.67649841308594
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 19 grad norm = 125.26571655273438
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 20 grad norm = 105.42816925048828
2023-09-14 23:40:06 | INFO | fairseq.trainer | decoder layer 21 grad norm = 120.34581756591797
2023-09-14 23:40:07 | INFO | fairseq.trainer | decoder layer 22 grad norm = 120.41451263427734
2023-09-14 23:40:07 | INFO | fairseq.trainer | decoder layer 23 grad norm = 272.0495910644531
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 0 total_norm 3630.761474609375
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 1 total_norm 2840.773681640625
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 2 total_norm 1970.5640869140625
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 3 total_norm 1710.61572265625
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 4 total_norm 1407.7044677734375
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 5 total_norm 1166.6112060546875
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 6 total_norm 749.8980102539062
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 7 total_norm 507.0394287109375
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 8 total_norm 380.11761474609375
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 9 total_norm 270.3086242675781
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 10 total_norm 206.04443359375
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 11 total_norm 169.83583068847656
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 12 total_norm 181.17218017578125
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 13 total_norm 151.05743408203125
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 14 total_norm 145.79908752441406
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 15 total_norm 152.59828186035156
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 16 total_norm 146.611328125
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 17 total_norm 163.74676513671875
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 18 total_norm 144.67649841308594
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 19 total_norm 125.26571655273438
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 20 total_norm 105.42816925048828
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 21 total_norm 120.34581756591797
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 22 total_norm 120.41451263427734
2023-09-14 23:40:07 | INFO | fairseq.trainer | layer 23 total_norm 272.0495910644531
2023-09-14 23:40:07 | INFO | train_inner | epoch 001:      3 / 32 loss=444.932, sample_size=32, ntokens=33, wps=5.6, ups=0.17, wpb=33, bsz=1, num_updates=3, lr=0.003, gnorm=36.153, clip=100, train_wall=6, wall=19
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 0 grad norm = 4896.2392578125
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 1 grad norm = 3881.826904296875
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2491.779541015625
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 3 grad norm = 2031.5576171875
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1578.513671875
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1369.8233642578125
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1164.1871337890625
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 7 grad norm = 794.5299072265625
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 8 grad norm = 574.5458984375
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 9 grad norm = 401.0074157714844
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 10 grad norm = 311.7818908691406
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 11 grad norm = 241.88134765625
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 12 grad norm = 250.3625030517578
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 13 grad norm = 208.2373809814453
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 14 grad norm = 194.5634002685547
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 15 grad norm = 222.43911743164062
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 16 grad norm = 212.65338134765625
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 17 grad norm = 221.05398559570312
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 18 grad norm = 200.8645782470703
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 19 grad norm = 188.93370056152344
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 20 grad norm = 162.32705688476562
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 21 grad norm = 174.5784149169922
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 22 grad norm = 188.66847229003906
2023-09-14 23:40:13 | INFO | fairseq.trainer | decoder layer 23 grad norm = 362.2576904296875
2023-09-14 23:40:13 | INFO | fairseq.trainer | layer 0 total_norm 4896.2392578125
2023-09-14 23:40:13 | INFO | fairseq.trainer | layer 1 total_norm 3881.826904296875
2023-09-14 23:40:13 | INFO | fairseq.trainer | layer 2 total_norm 2491.779541015625
2023-09-14 23:40:13 | INFO | fairseq.trainer | layer 3 total_norm 2031.5576171875
2023-09-14 23:40:13 | INFO | fairseq.trainer | layer 4 total_norm 1578.513671875
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 5 total_norm 1369.8233642578125
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 6 total_norm 1164.1871337890625
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 7 total_norm 794.5299072265625
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 8 total_norm 574.5458984375
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 9 total_norm 401.0074157714844
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 10 total_norm 311.7818908691406
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 11 total_norm 241.88134765625
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 12 total_norm 250.3625030517578
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 13 total_norm 208.2373809814453
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 14 total_norm 194.5634002685547
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 15 total_norm 222.43911743164062
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 16 total_norm 212.65338134765625
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 17 total_norm 221.05398559570312
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 18 total_norm 200.8645782470703
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 19 total_norm 188.93370056152344
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 20 total_norm 162.32705688476562
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 21 total_norm 174.5784149169922
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 22 total_norm 188.66847229003906
2023-09-14 23:40:14 | INFO | fairseq.trainer | layer 23 total_norm 362.2576904296875
2023-09-14 23:40:14 | INFO | train_inner | epoch 001:      4 / 32 loss=450.495, sample_size=48, ntokens=49, wps=7.5, ups=0.15, wpb=49, bsz=1, num_updates=4, lr=0.003, gnorm=34.369, clip=100, train_wall=6, wall=25
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2731.8564453125
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2192.697998046875
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1610.5262451171875
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1404.1702880859375
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1146.9796142578125
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1020.3922729492188
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1023.6182861328125
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 7 grad norm = 905.5620727539062
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 8 grad norm = 655.3814697265625
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 9 grad norm = 454.1441955566406
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 10 grad norm = 345.1891784667969
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 11 grad norm = 271.784423828125
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 12 grad norm = 276.13568115234375
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 13 grad norm = 234.56399536132812
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 14 grad norm = 216.64801025390625
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 15 grad norm = 261.0099792480469
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 16 grad norm = 236.7195587158203
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 17 grad norm = 248.57992553710938
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 18 grad norm = 224.7327423095703
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 19 grad norm = 212.44955444335938
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 20 grad norm = 182.5635528564453
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 21 grad norm = 205.96006774902344
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 22 grad norm = 208.3065643310547
2023-09-14 23:40:18 | INFO | fairseq.trainer | decoder layer 23 grad norm = 435.8858337402344
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 0 total_norm 2731.8564453125
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 1 total_norm 2192.697998046875
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 2 total_norm 1610.5262451171875
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 3 total_norm 1404.1702880859375
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 4 total_norm 1146.9796142578125
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 5 total_norm 1020.3922729492188
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 6 total_norm 1023.6182861328125
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 7 total_norm 905.5620727539062
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 8 total_norm 655.3814697265625
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 9 total_norm 454.1441955566406
2023-09-14 23:40:18 | INFO | fairseq.trainer | layer 10 total_norm 345.1891784667969
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 11 total_norm 271.784423828125
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 12 total_norm 276.13568115234375
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 13 total_norm 234.56399536132812
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 14 total_norm 216.64801025390625
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 15 total_norm 261.0099792480469
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 16 total_norm 236.7195587158203
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 17 total_norm 248.57992553710938
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 18 total_norm 224.7327423095703
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 19 total_norm 212.44955444335938
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 20 total_norm 182.5635528564453
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 21 total_norm 205.96006774902344
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 22 total_norm 208.3065643310547
2023-09-14 23:40:19 | INFO | fairseq.trainer | layer 23 total_norm 435.8858337402344
2023-09-14 23:40:19 | INFO | train_inner | epoch 001:      5 / 32 loss=480.211, sample_size=21, ntokens=22, wps=4.5, ups=0.21, wpb=22, bsz=1, num_updates=5, lr=0.003, gnorm=39.945, clip=100, train_wall=5, wall=30
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2461.1279296875
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2043.9718017578125
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1510.7257080078125
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1298.6912841796875
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1062.3802490234375
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 5 grad norm = 973.0991821289062
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 6 grad norm = 992.5010375976562
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 7 grad norm = 976.7304077148438
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 8 grad norm = 738.5933227539062
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 9 grad norm = 509.2160339355469
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 10 grad norm = 380.9071960449219
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 11 grad norm = 297.3805847167969
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 12 grad norm = 307.0704040527344
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 13 grad norm = 258.41131591796875
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 14 grad norm = 240.52210998535156
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 15 grad norm = 280.42657470703125
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 16 grad norm = 262.56756591796875
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 17 grad norm = 278.8970031738281
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 18 grad norm = 245.01577758789062
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 19 grad norm = 230.8619384765625
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 20 grad norm = 201.80267333984375
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 21 grad norm = 235.1741485595703
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 22 grad norm = 251.50094604492188
2023-09-14 23:40:23 | INFO | fairseq.trainer | decoder layer 23 grad norm = 514.6094970703125
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 0 total_norm 2461.1279296875
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 1 total_norm 2043.9718017578125
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 2 total_norm 1510.7257080078125
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 3 total_norm 1298.6912841796875
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 4 total_norm 1062.3802490234375
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 5 total_norm 973.0991821289062
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 6 total_norm 992.5010375976562
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 7 total_norm 976.7304077148438
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 8 total_norm 738.5933227539062
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 9 total_norm 509.2160339355469
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 10 total_norm 380.9071960449219
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 11 total_norm 297.3805847167969
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 12 total_norm 307.0704040527344
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 13 total_norm 258.41131591796875
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 14 total_norm 240.52210998535156
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 15 total_norm 280.42657470703125
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 16 total_norm 262.56756591796875
2023-09-14 23:40:23 | INFO | fairseq.trainer | layer 17 total_norm 278.8970031738281
2023-09-14 23:40:24 | INFO | fairseq.trainer | layer 18 total_norm 245.01577758789062
2023-09-14 23:40:24 | INFO | fairseq.trainer | layer 19 total_norm 230.8619384765625
2023-09-14 23:40:24 | INFO | fairseq.trainer | layer 20 total_norm 201.80267333984375
2023-09-14 23:40:24 | INFO | fairseq.trainer | layer 21 total_norm 235.1741485595703
2023-09-14 23:40:24 | INFO | fairseq.trainer | layer 22 total_norm 251.50094604492188
2023-09-14 23:40:24 | INFO | fairseq.trainer | layer 23 total_norm 514.6094970703125
2023-09-14 23:40:24 | INFO | train_inner | epoch 001:      6 / 32 loss=482.18, sample_size=19, ntokens=20, wps=4.1, ups=0.21, wpb=20, bsz=1, num_updates=6, lr=0.003, gnorm=37.767, clip=100, train_wall=5, wall=35
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 0 grad norm = 4761.1396484375
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 1 grad norm = 3736.0087890625
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2467.276611328125
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1975.8770751953125
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1512.1441650390625
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1378.5062255859375
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1231.1201171875
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1136.0692138671875
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 8 grad norm = 904.0416870117188
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 9 grad norm = 623.8429565429688
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 10 grad norm = 450.71673583984375
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 11 grad norm = 346.02105712890625
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 12 grad norm = 354.0366516113281
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 13 grad norm = 293.6990661621094
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 14 grad norm = 281.5243225097656
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 15 grad norm = 321.4233703613281
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 16 grad norm = 311.05902099609375
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 17 grad norm = 324.5100402832031
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 18 grad norm = 292.1480712890625
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 19 grad norm = 283.90777587890625
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 20 grad norm = 242.4395751953125
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 21 grad norm = 288.0098876953125
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 22 grad norm = 306.87005615234375
2023-09-14 23:40:29 | INFO | fairseq.trainer | decoder layer 23 grad norm = 597.5084838867188
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 0 total_norm 4761.1396484375
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 1 total_norm 3736.0087890625
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 2 total_norm 2467.276611328125
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 3 total_norm 1975.8770751953125
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 4 total_norm 1512.1441650390625
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 5 total_norm 1378.5062255859375
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 6 total_norm 1231.1201171875
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 7 total_norm 1136.0692138671875
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 8 total_norm 904.0416870117188
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 9 total_norm 623.8429565429688
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 10 total_norm 450.71673583984375
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 11 total_norm 346.02105712890625
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 12 total_norm 354.0366516113281
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 13 total_norm 293.6990661621094
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 14 total_norm 281.5243225097656
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 15 total_norm 321.4233703613281
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 16 total_norm 311.05902099609375
2023-09-14 23:40:29 | INFO | fairseq.trainer | layer 17 total_norm 324.5100402832031
2023-09-14 23:40:30 | INFO | fairseq.trainer | layer 18 total_norm 292.1480712890625
2023-09-14 23:40:30 | INFO | fairseq.trainer | layer 19 total_norm 283.90777587890625
2023-09-14 23:40:30 | INFO | fairseq.trainer | layer 20 total_norm 242.4395751953125
2023-09-14 23:40:30 | INFO | fairseq.trainer | layer 21 total_norm 288.0098876953125
2023-09-14 23:40:30 | INFO | fairseq.trainer | layer 22 total_norm 306.87005615234375
2023-09-14 23:40:30 | INFO | fairseq.trainer | layer 23 total_norm 597.5084838867188
2023-09-14 23:40:30 | INFO | train_inner | epoch 001:      7 / 32 loss=479.439, sample_size=45, ntokens=46, wps=7.7, ups=0.17, wpb=46, bsz=1, num_updates=7, lr=0.003, gnorm=37.299, clip=100, train_wall=6, wall=41
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 0 grad norm = 4424.54833984375
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 1 grad norm = 3434.572998046875
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2311.895751953125
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1908.137939453125
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1451.0233154296875
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1295.549560546875
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1161.7694091796875
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1087.4461669921875
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1056.1954345703125
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 9 grad norm = 716.7821044921875
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 10 grad norm = 505.62554931640625
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 11 grad norm = 387.8445129394531
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 12 grad norm = 395.30157470703125
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 13 grad norm = 329.8704833984375
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 14 grad norm = 315.8683166503906
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 15 grad norm = 369.6778259277344
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 16 grad norm = 362.50103759765625
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 17 grad norm = 373.4937744140625
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 18 grad norm = 337.8442687988281
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 19 grad norm = 328.017578125
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 20 grad norm = 275.8066711425781
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 21 grad norm = 336.3356018066406
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 22 grad norm = 363.9881591796875
2023-09-14 23:40:35 | INFO | fairseq.trainer | decoder layer 23 grad norm = 682.4136352539062
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 0 total_norm 4424.54833984375
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 1 total_norm 3434.572998046875
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 2 total_norm 2311.895751953125
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 3 total_norm 1908.137939453125
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 4 total_norm 1451.0233154296875
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 5 total_norm 1295.549560546875
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 6 total_norm 1161.7694091796875
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 7 total_norm 1087.4461669921875
2023-09-14 23:40:35 | INFO | fairseq.trainer | layer 8 total_norm 1056.1954345703125
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 9 total_norm 716.7821044921875
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 10 total_norm 505.62554931640625
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 11 total_norm 387.8445129394531
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 12 total_norm 395.30157470703125
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 13 total_norm 329.8704833984375
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 14 total_norm 315.8683166503906
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 15 total_norm 369.6778259277344
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 16 total_norm 362.50103759765625
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 17 total_norm 373.4937744140625
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 18 total_norm 337.8442687988281
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 19 total_norm 328.017578125
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 20 total_norm 275.8066711425781
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 21 total_norm 336.3356018066406
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 22 total_norm 363.9881591796875
2023-09-14 23:40:36 | INFO | fairseq.trainer | layer 23 total_norm 682.4136352539062
2023-09-14 23:40:36 | INFO | train_inner | epoch 001:      8 / 32 loss=442.071, sample_size=44, ntokens=45, wps=7.3, ups=0.16, wpb=45, bsz=1, num_updates=8, lr=0.003, gnorm=34.033, clip=100, train_wall=6, wall=47
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 0 grad norm = 4209.77734375
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 1 grad norm = 3219.7783203125
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2161.54443359375
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1782.062255859375
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1354.2650146484375
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1225.4697265625
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1108.357666015625
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1065.6580810546875
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1042.9471435546875
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 9 grad norm = 784.2600708007812
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 10 grad norm = 548.0420532226562
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 11 grad norm = 415.6138610839844
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 12 grad norm = 415.55645751953125
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 13 grad norm = 352.7742919921875
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 14 grad norm = 340.887939453125
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 15 grad norm = 392.17462158203125
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 16 grad norm = 407.8909912109375
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 17 grad norm = 409.5928039550781
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 18 grad norm = 360.796875
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 19 grad norm = 349.63427734375
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 20 grad norm = 300.4355163574219
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 21 grad norm = 365.1255187988281
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 22 grad norm = 400.9211730957031
2023-09-14 23:40:41 | INFO | fairseq.trainer | decoder layer 23 grad norm = 760.441162109375
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 0 total_norm 4209.77734375
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 1 total_norm 3219.7783203125
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 2 total_norm 2161.54443359375
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 3 total_norm 1782.062255859375
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 4 total_norm 1354.2650146484375
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 5 total_norm 1225.4697265625
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 6 total_norm 1108.357666015625
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 7 total_norm 1065.6580810546875
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 8 total_norm 1042.9471435546875
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 9 total_norm 784.2600708007812
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 10 total_norm 548.0420532226562
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 11 total_norm 415.6138610839844
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 12 total_norm 415.55645751953125
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 13 total_norm 352.7742919921875
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 14 total_norm 340.887939453125
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 15 total_norm 392.17462158203125
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 16 total_norm 407.8909912109375
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 17 total_norm 409.5928039550781
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 18 total_norm 360.796875
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 19 total_norm 349.63427734375
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 20 total_norm 300.4355163574219
2023-09-14 23:40:41 | INFO | fairseq.trainer | layer 21 total_norm 365.1255187988281
2023-09-14 23:40:42 | INFO | fairseq.trainer | layer 22 total_norm 400.9211730957031
2023-09-14 23:40:42 | INFO | fairseq.trainer | layer 23 total_norm 760.441162109375
2023-09-14 23:40:42 | INFO | train_inner | epoch 001:      9 / 32 loss=432.452, sample_size=39, ntokens=40, wps=7, ups=0.17, wpb=40, bsz=1, num_updates=9, lr=0.003, gnorm=35.496, clip=100, train_wall=6, wall=53
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3118.73681640625
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2501.278076171875
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1799.7435302734375
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1459.5316162109375
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1169.017822265625
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1046.4251708984375
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1008.8764038085938
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 7 grad norm = 996.2655639648438
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1013.8662109375
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 9 grad norm = 842.1776123046875
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 10 grad norm = 583.3341064453125
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 11 grad norm = 438.6101379394531
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 12 grad norm = 439.16802978515625
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 13 grad norm = 377.864013671875
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 14 grad norm = 368.34539794921875
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 15 grad norm = 423.16363525390625
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 16 grad norm = 436.6640625
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 17 grad norm = 434.11761474609375
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 18 grad norm = 379.0133972167969
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 19 grad norm = 362.2673034667969
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 20 grad norm = 319.5907287597656
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 21 grad norm = 372.5829772949219
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 22 grad norm = 422.95672607421875
2023-09-14 23:40:46 | INFO | fairseq.trainer | decoder layer 23 grad norm = 835.182861328125
2023-09-14 23:40:46 | INFO | fairseq.trainer | layer 0 total_norm 3118.73681640625
2023-09-14 23:40:46 | INFO | fairseq.trainer | layer 1 total_norm 2501.278076171875
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 2 total_norm 1799.7435302734375
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 3 total_norm 1459.5316162109375
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 4 total_norm 1169.017822265625
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 5 total_norm 1046.4251708984375
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 6 total_norm 1008.8764038085938
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 7 total_norm 996.2655639648438
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 8 total_norm 1013.8662109375
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 9 total_norm 842.1776123046875
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 10 total_norm 583.3341064453125
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 11 total_norm 438.6101379394531
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 12 total_norm 439.16802978515625
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 13 total_norm 377.864013671875
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 14 total_norm 368.34539794921875
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 15 total_norm 423.16363525390625
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 16 total_norm 436.6640625
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 17 total_norm 434.11761474609375
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 18 total_norm 379.0133972167969
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 19 total_norm 362.2673034667969
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 20 total_norm 319.5907287597656
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 21 total_norm 372.5829772949219
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 22 total_norm 422.95672607421875
2023-09-14 23:40:47 | INFO | fairseq.trainer | layer 23 total_norm 835.182861328125
2023-09-14 23:40:47 | INFO | train_inner | epoch 001:     10 / 32 loss=409.22, sample_size=28, ntokens=29, wps=5.4, ups=0.19, wpb=29, bsz=1, num_updates=10, lr=0.003, gnorm=35.134, clip=100, train_wall=5, wall=58
2023-09-14 23:40:51 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3239.157958984375
2023-09-14 23:40:51 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2614.39111328125
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1859.0120849609375
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1488.135986328125
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1190.3306884765625
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1085.6917724609375
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1035.0731201171875
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1004.9723510742188
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1028.4359130859375
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 9 grad norm = 902.565673828125
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 10 grad norm = 618.4313354492188
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 11 grad norm = 463.8308410644531
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 12 grad norm = 461.40936279296875
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 13 grad norm = 395.7624816894531
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 14 grad norm = 394.25213623046875
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 15 grad norm = 460.5152282714844
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 16 grad norm = 476.7738952636719
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 17 grad norm = 469.2362060546875
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 18 grad norm = 394.28857421875
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 19 grad norm = 366.18792724609375
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 20 grad norm = 332.69390869140625
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 21 grad norm = 385.92254638671875
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 22 grad norm = 453.8687438964844
2023-09-14 23:40:52 | INFO | fairseq.trainer | decoder layer 23 grad norm = 914.3807373046875
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 0 total_norm 3239.157958984375
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 1 total_norm 2614.39111328125
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 2 total_norm 1859.0120849609375
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 3 total_norm 1488.135986328125
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 4 total_norm 1190.3306884765625
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 5 total_norm 1085.6917724609375
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 6 total_norm 1035.0731201171875
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 7 total_norm 1004.9723510742188
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 8 total_norm 1028.4359130859375
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 9 total_norm 902.565673828125
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 10 total_norm 618.4313354492188
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 11 total_norm 463.8308410644531
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 12 total_norm 461.40936279296875
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 13 total_norm 395.7624816894531
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 14 total_norm 394.25213623046875
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 15 total_norm 460.5152282714844
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 16 total_norm 476.7738952636719
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 17 total_norm 469.2362060546875
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 18 total_norm 394.28857421875
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 19 total_norm 366.18792724609375
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 20 total_norm 332.69390869140625
2023-09-14 23:40:52 | INFO | fairseq.trainer | layer 21 total_norm 385.92254638671875
2023-09-14 23:40:53 | INFO | fairseq.trainer | layer 22 total_norm 453.8687438964844
2023-09-14 23:40:53 | INFO | fairseq.trainer | layer 23 total_norm 914.3807373046875
2023-09-14 23:40:53 | INFO | train_inner | epoch 001:     11 / 32 loss=428.733, sample_size=29, ntokens=30, wps=5.4, ups=0.18, wpb=30, bsz=1, num_updates=11, lr=0.003, gnorm=35.975, clip=100, train_wall=6, wall=64
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 0 grad norm = 5591.873046875
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 1 grad norm = 4356.86083984375
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2887.663818359375
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 3 grad norm = 2177.33447265625
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1564.2908935546875
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1394.77490234375
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1184.4471435546875
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1106.0416259765625
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1100.468505859375
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 9 grad norm = 944.4644775390625
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 10 grad norm = 675.3742065429688
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 11 grad norm = 507.85247802734375
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 12 grad norm = 506.2277526855469
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 13 grad norm = 435.3838195800781
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 14 grad norm = 437.3547668457031
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 15 grad norm = 511.4564208984375
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 16 grad norm = 526.3125610351562
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 17 grad norm = 512.7290649414062
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 18 grad norm = 429.6739501953125
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 19 grad norm = 404.0869445800781
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 20 grad norm = 367.64947509765625
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 21 grad norm = 423.7701110839844
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 22 grad norm = 475.31451416015625
2023-09-14 23:40:59 | INFO | fairseq.trainer | decoder layer 23 grad norm = 986.3280639648438
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 0 total_norm 5591.873046875
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 1 total_norm 4356.86083984375
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 2 total_norm 2887.663818359375
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 3 total_norm 2177.33447265625
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 4 total_norm 1564.2908935546875
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 5 total_norm 1394.77490234375
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 6 total_norm 1184.4471435546875
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 7 total_norm 1106.0416259765625
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 8 total_norm 1100.468505859375
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 9 total_norm 944.4644775390625
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 10 total_norm 675.3742065429688
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 11 total_norm 507.85247802734375
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 12 total_norm 506.2277526855469
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 13 total_norm 435.3838195800781
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 14 total_norm 437.3547668457031
2023-09-14 23:40:59 | INFO | fairseq.trainer | layer 15 total_norm 511.4564208984375
2023-09-14 23:41:00 | INFO | fairseq.trainer | layer 16 total_norm 526.3125610351562
2023-09-14 23:41:00 | INFO | fairseq.trainer | layer 17 total_norm 512.7290649414062
2023-09-14 23:41:00 | INFO | fairseq.trainer | layer 18 total_norm 429.6739501953125
2023-09-14 23:41:00 | INFO | fairseq.trainer | layer 19 total_norm 404.0869445800781
2023-09-14 23:41:00 | INFO | fairseq.trainer | layer 20 total_norm 367.64947509765625
2023-09-14 23:41:00 | INFO | fairseq.trainer | layer 21 total_norm 423.7701110839844
2023-09-14 23:41:00 | INFO | fairseq.trainer | layer 22 total_norm 475.31451416015625
2023-09-14 23:41:00 | INFO | fairseq.trainer | layer 23 total_norm 986.3280639648438
2023-09-14 23:41:00 | INFO | train_inner | epoch 001:     12 / 32 loss=414.842, sample_size=59, ntokens=60, wps=8.4, ups=0.14, wpb=60, bsz=1, num_updates=12, lr=0.003, gnorm=33.158, clip=100, train_wall=7, wall=71
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 0 grad norm = 4951.80029296875
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 1 grad norm = 3665.734130859375
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2482.43994140625
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1892.0643310546875
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1434.6351318359375
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1325.8607177734375
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1165.511962890625
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1099.3670654296875
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1056.321044921875
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 9 grad norm = 931.6490478515625
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 10 grad norm = 712.7982788085938
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 11 grad norm = 531.2899780273438
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 12 grad norm = 527.934326171875
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 13 grad norm = 455.6929626464844
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 14 grad norm = 468.765380859375
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 15 grad norm = 537.2669067382812
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 16 grad norm = 573.4456787109375
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 17 grad norm = 552.7783813476562
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 18 grad norm = 459.0899353027344
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 19 grad norm = 420.6726379394531
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 20 grad norm = 398.86822509765625
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 21 grad norm = 458.8517150878906
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 22 grad norm = 520.1726684570312
2023-09-14 23:41:05 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1066.3692626953125
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 0 total_norm 4951.80029296875
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 1 total_norm 3665.734130859375
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 2 total_norm 2482.43994140625
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 3 total_norm 1892.0643310546875
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 4 total_norm 1434.6351318359375
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 5 total_norm 1325.8607177734375
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 6 total_norm 1165.511962890625
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 7 total_norm 1099.3670654296875
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 8 total_norm 1056.321044921875
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 9 total_norm 931.6490478515625
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 10 total_norm 712.7982788085938
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 11 total_norm 531.2899780273438
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 12 total_norm 527.934326171875
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 13 total_norm 455.6929626464844
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 14 total_norm 468.765380859375
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 15 total_norm 537.2669067382812
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 16 total_norm 573.4456787109375
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 17 total_norm 552.7783813476562
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 18 total_norm 459.0899353027344
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 19 total_norm 420.6726379394531
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 20 total_norm 398.86822509765625
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 21 total_norm 458.8517150878906
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 22 total_norm 520.1726684570312
2023-09-14 23:41:06 | INFO | fairseq.trainer | layer 23 total_norm 1066.3692626953125
2023-09-14 23:41:06 | INFO | train_inner | epoch 001:     13 / 32 loss=458.852, sample_size=46, ntokens=47, wps=7.4, ups=0.16, wpb=47, bsz=1, num_updates=13, lr=0.003, gnorm=36.73, clip=100, train_wall=6, wall=77
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3546.256103515625
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2716.684814453125
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2005.0048828125
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1571.5191650390625
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1246.004638671875
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1164.3785400390625
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1057.2994384765625
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1022.6568603515625
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1020.39892578125
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 9 grad norm = 907.4462280273438
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 10 grad norm = 743.1203002929688
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 11 grad norm = 553.5016479492188
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 12 grad norm = 549.3154907226562
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 13 grad norm = 476.142822265625
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 14 grad norm = 495.3110656738281
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 15 grad norm = 555.1026000976562
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 16 grad norm = 602.1738891601562
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 17 grad norm = 582.9935302734375
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 18 grad norm = 489.69171142578125
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 19 grad norm = 457.70758056640625
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 20 grad norm = 427.95904541015625
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 21 grad norm = 486.6020202636719
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 22 grad norm = 559.6957397460938
2023-09-14 23:41:11 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1140.6243896484375
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 0 total_norm 3546.256103515625
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 1 total_norm 2716.684814453125
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 2 total_norm 2005.0048828125
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 3 total_norm 1571.5191650390625
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 4 total_norm 1246.004638671875
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 5 total_norm 1164.3785400390625
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 6 total_norm 1057.2994384765625
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 7 total_norm 1022.6568603515625
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 8 total_norm 1020.39892578125
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 9 total_norm 907.4462280273438
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 10 total_norm 743.1203002929688
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 11 total_norm 553.5016479492188
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 12 total_norm 549.3154907226562
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 13 total_norm 476.142822265625
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 14 total_norm 495.3110656738281
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 15 total_norm 555.1026000976562
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 16 total_norm 602.1738891601562
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 17 total_norm 582.9935302734375
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 18 total_norm 489.69171142578125
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 19 total_norm 457.70758056640625
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 20 total_norm 427.95904541015625
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 21 total_norm 486.6020202636719
2023-09-14 23:41:11 | INFO | fairseq.trainer | layer 22 total_norm 559.6957397460938
2023-09-14 23:41:12 | INFO | fairseq.trainer | layer 23 total_norm 1140.6243896484375
2023-09-14 23:41:12 | INFO | train_inner | epoch 001:     14 / 32 loss=475.229, sample_size=30, ntokens=31, wps=5.7, ups=0.18, wpb=31, bsz=1, num_updates=14, lr=0.003, gnorm=41.899, clip=100, train_wall=5, wall=83
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3403.90185546875
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2536.631591796875
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1898.2713623046875
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1452.7330322265625
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1158.0157470703125
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1088.3662109375
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1024.8126220703125
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1004.4248046875
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1015.9998168945312
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 9 grad norm = 906.6943969726562
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 10 grad norm = 776.7666015625
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 11 grad norm = 576.1672973632812
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 12 grad norm = 568.0228271484375
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 13 grad norm = 496.80194091796875
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 14 grad norm = 516.6258544921875
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 15 grad norm = 576.5752563476562
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 16 grad norm = 636.278564453125
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 17 grad norm = 614.398193359375
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 18 grad norm = 509.32281494140625
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 19 grad norm = 462.0871276855469
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 20 grad norm = 445.9854431152344
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 21 grad norm = 497.322998046875
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 22 grad norm = 599.992919921875
2023-09-14 23:41:16 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1213.6632080078125
2023-09-14 23:41:16 | INFO | fairseq.trainer | layer 0 total_norm 3403.90185546875
2023-09-14 23:41:16 | INFO | fairseq.trainer | layer 1 total_norm 2536.631591796875
2023-09-14 23:41:16 | INFO | fairseq.trainer | layer 2 total_norm 1898.2713623046875
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 3 total_norm 1452.7330322265625
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 4 total_norm 1158.0157470703125
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 5 total_norm 1088.3662109375
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 6 total_norm 1024.8126220703125
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 7 total_norm 1004.4248046875
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 8 total_norm 1015.9998168945312
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 9 total_norm 906.6943969726562
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 10 total_norm 776.7666015625
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 11 total_norm 576.1672973632812
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 12 total_norm 568.0228271484375
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 13 total_norm 496.80194091796875
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 14 total_norm 516.6258544921875
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 15 total_norm 576.5752563476562
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 16 total_norm 636.278564453125
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 17 total_norm 614.398193359375
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 18 total_norm 509.32281494140625
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 19 total_norm 462.0871276855469
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 20 total_norm 445.9854431152344
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 21 total_norm 497.322998046875
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 22 total_norm 599.992919921875
2023-09-14 23:41:17 | INFO | fairseq.trainer | layer 23 total_norm 1213.6632080078125
2023-09-14 23:41:17 | INFO | train_inner | epoch 001:     15 / 32 loss=453.799, sample_size=29, ntokens=30, wps=5.4, ups=0.18, wpb=30, bsz=1, num_updates=15, lr=0.003, gnorm=38.566, clip=100, train_wall=5, wall=88
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2251.79296875
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 1 grad norm = 1847.0655517578125
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1483.8084716796875
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1200.563720703125
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 4 grad norm = 986.9081420898438
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 5 grad norm = 937.4122314453125
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 6 grad norm = 939.1007080078125
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 7 grad norm = 948.8340454101562
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 8 grad norm = 983.7196044921875
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 9 grad norm = 883.4966430664062
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 10 grad norm = 795.2520141601562
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 11 grad norm = 591.1079711914062
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 12 grad norm = 578.5093994140625
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 13 grad norm = 507.3531494140625
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 14 grad norm = 529.7626342773438
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 15 grad norm = 581.09228515625
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 16 grad norm = 660.9375
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 17 grad norm = 640.8770141601562
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 18 grad norm = 527.1359252929688
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 19 grad norm = 478.4989929199219
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 20 grad norm = 462.9610900878906
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 21 grad norm = 516.5597534179688
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 22 grad norm = 619.8479614257812
2023-09-14 23:41:21 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1285.328125
2023-09-14 23:41:21 | INFO | fairseq.trainer | layer 0 total_norm 2251.79296875
2023-09-14 23:41:21 | INFO | fairseq.trainer | layer 1 total_norm 1847.0655517578125
2023-09-14 23:41:21 | INFO | fairseq.trainer | layer 2 total_norm 1483.8084716796875
2023-09-14 23:41:21 | INFO | fairseq.trainer | layer 3 total_norm 1200.563720703125
2023-09-14 23:41:21 | INFO | fairseq.trainer | layer 4 total_norm 986.9081420898438
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 5 total_norm 937.4122314453125
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 6 total_norm 939.1007080078125
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 7 total_norm 948.8340454101562
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 8 total_norm 983.7196044921875
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 9 total_norm 883.4966430664062
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 10 total_norm 795.2520141601562
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 11 total_norm 591.1079711914062
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 12 total_norm 578.5093994140625
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 13 total_norm 507.3531494140625
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 14 total_norm 529.7626342773438
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 15 total_norm 581.09228515625
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 16 total_norm 660.9375
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 17 total_norm 640.8770141601562
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 18 total_norm 527.1359252929688
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 19 total_norm 478.4989929199219
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 20 total_norm 462.9610900878906
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 21 total_norm 516.5597534179688
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 22 total_norm 619.8479614257812
2023-09-14 23:41:22 | INFO | fairseq.trainer | layer 23 total_norm 1285.328125
2023-09-14 23:41:22 | INFO | train_inner | epoch 001:     16 / 32 loss=386.706, sample_size=20, ntokens=21, wps=4.3, ups=0.2, wpb=21, bsz=1, num_updates=16, lr=0.003, gnorm=34.72, clip=100, train_wall=5, wall=93
2023-09-14 23:41:26 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3217.931396484375
2023-09-14 23:41:26 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2599.490234375
2023-09-14 23:41:26 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1963.5107421875
2023-09-14 23:41:26 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1471.7449951171875
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1185.7645263671875
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1085.3048095703125
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1006.7711181640625
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 7 grad norm = 991.3421020507812
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1013.1439208984375
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 9 grad norm = 906.4186401367188
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 10 grad norm = 828.0811767578125
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 11 grad norm = 615.1734008789062
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 12 grad norm = 603.6513671875
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 13 grad norm = 530.5413818359375
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 14 grad norm = 551.46484375
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 15 grad norm = 597.9873046875
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 16 grad norm = 683.9066772460938
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 17 grad norm = 665.2674560546875
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 18 grad norm = 546.6624755859375
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 19 grad norm = 493.49468994140625
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 20 grad norm = 479.6375732421875
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 21 grad norm = 526.7870483398438
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 22 grad norm = 631.0609741210938
2023-09-14 23:41:27 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1355.7069091796875
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 0 total_norm 3217.931396484375
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 1 total_norm 2599.490234375
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 2 total_norm 1963.5107421875
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 3 total_norm 1471.7449951171875
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 4 total_norm 1185.7645263671875
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 5 total_norm 1085.3048095703125
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 6 total_norm 1006.7711181640625
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 7 total_norm 991.3421020507812
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 8 total_norm 1013.1439208984375
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 9 total_norm 906.4186401367188
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 10 total_norm 828.0811767578125
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 11 total_norm 615.1734008789062
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 12 total_norm 603.6513671875
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 13 total_norm 530.5413818359375
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 14 total_norm 551.46484375
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 15 total_norm 597.9873046875
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 16 total_norm 683.9066772460938
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 17 total_norm 665.2674560546875
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 18 total_norm 546.6624755859375
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 19 total_norm 493.49468994140625
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 20 total_norm 479.6375732421875
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 21 total_norm 526.7870483398438
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 22 total_norm 631.0609741210938
2023-09-14 23:41:27 | INFO | fairseq.trainer | layer 23 total_norm 1355.7069091796875
2023-09-14 23:41:27 | INFO | train_inner | epoch 001:     17 / 32 loss=401.363, sample_size=31, ntokens=32, wps=5.8, ups=0.18, wpb=32, bsz=1, num_updates=17, lr=0.003, gnorm=36.177, clip=100, train_wall=5, wall=99
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3425.486572265625
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2713.483154296875
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2104.556640625
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1513.95263671875
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1233.9871826171875
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1145.926025390625
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1040.1258544921875
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1012.0685424804688
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1009.9811401367188
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 9 grad norm = 896.3185424804688
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 10 grad norm = 851.8270874023438
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 11 grad norm = 631.579345703125
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 12 grad norm = 619.5562133789062
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 13 grad norm = 544.578369140625
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 14 grad norm = 573.900390625
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 15 grad norm = 615.7195434570312
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 16 grad norm = 712.6248168945312
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 17 grad norm = 692.7459106445312
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 18 grad norm = 568.8900756835938
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 19 grad norm = 515.3750610351562
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 20 grad norm = 502.84710693359375
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 21 grad norm = 547.3527221679688
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 22 grad norm = 644.4021606445312
2023-09-14 23:41:32 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1426.331787109375
2023-09-14 23:41:32 | INFO | fairseq.trainer | layer 0 total_norm 3425.486572265625
2023-09-14 23:41:32 | INFO | fairseq.trainer | layer 1 total_norm 2713.483154296875
2023-09-14 23:41:32 | INFO | fairseq.trainer | layer 2 total_norm 2104.556640625
2023-09-14 23:41:32 | INFO | fairseq.trainer | layer 3 total_norm 1513.95263671875
2023-09-14 23:41:32 | INFO | fairseq.trainer | layer 4 total_norm 1233.9871826171875
2023-09-14 23:41:32 | INFO | fairseq.trainer | layer 5 total_norm 1145.926025390625
2023-09-14 23:41:32 | INFO | fairseq.trainer | layer 6 total_norm 1040.1258544921875
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 7 total_norm 1012.0685424804688
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 8 total_norm 1009.9811401367188
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 9 total_norm 896.3185424804688
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 10 total_norm 851.8270874023438
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 11 total_norm 631.579345703125
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 12 total_norm 619.5562133789062
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 13 total_norm 544.578369140625
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 14 total_norm 573.900390625
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 15 total_norm 615.7195434570312
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 16 total_norm 712.6248168945312
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 17 total_norm 692.7459106445312
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 18 total_norm 568.8900756835938
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 19 total_norm 515.3750610351562
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 20 total_norm 502.84710693359375
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 21 total_norm 547.3527221679688
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 22 total_norm 644.4021606445312
2023-09-14 23:41:33 | INFO | fairseq.trainer | layer 23 total_norm 1426.331787109375
2023-09-14 23:41:33 | INFO | train_inner | epoch 001:     18 / 32 loss=466.952, sample_size=31, ntokens=32, wps=5.9, ups=0.18, wpb=32, bsz=1, num_updates=18, lr=0.003, gnorm=40.347, clip=100, train_wall=5, wall=104
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 0 grad norm = 5699.0498046875
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 1 grad norm = 4387.19677734375
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 2 grad norm = 3295.1259765625
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 3 grad norm = 2060.04150390625
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1588.531982421875
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1411.4072265625
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1174.990966796875
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1094.97021484375
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1056.393310546875
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 9 grad norm = 921.6155395507812
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 10 grad norm = 894.4692993164062
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 11 grad norm = 662.2382202148438
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 12 grad norm = 648.0975952148438
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 13 grad norm = 572.2325439453125
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 14 grad norm = 615.2213134765625
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 15 grad norm = 655.7303466796875
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 16 grad norm = 769.9390258789062
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 17 grad norm = 742.4735107421875
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 18 grad norm = 600.5999145507812
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 19 grad norm = 535.02978515625
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 20 grad norm = 528.86083984375
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 21 grad norm = 551.653564453125
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 22 grad norm = 673.2896118164062
2023-09-14 23:41:39 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1500.986083984375
2023-09-14 23:41:39 | INFO | fairseq.trainer | layer 0 total_norm 5699.0498046875
2023-09-14 23:41:39 | INFO | fairseq.trainer | layer 1 total_norm 4387.19677734375
2023-09-14 23:41:39 | INFO | fairseq.trainer | layer 2 total_norm 3295.1259765625
2023-09-14 23:41:39 | INFO | fairseq.trainer | layer 3 total_norm 2060.04150390625
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 4 total_norm 1588.531982421875
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 5 total_norm 1411.4072265625
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 6 total_norm 1174.990966796875
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 7 total_norm 1094.97021484375
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 8 total_norm 1056.393310546875
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 9 total_norm 921.6155395507812
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 10 total_norm 894.4692993164062
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 11 total_norm 662.2382202148438
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 12 total_norm 648.0975952148438
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 13 total_norm 572.2325439453125
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 14 total_norm 615.2213134765625
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 15 total_norm 655.7303466796875
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 16 total_norm 769.9390258789062
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 17 total_norm 742.4735107421875
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 18 total_norm 600.5999145507812
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 19 total_norm 535.02978515625
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 20 total_norm 528.86083984375
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 21 total_norm 551.653564453125
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 22 total_norm 673.2896118164062
2023-09-14 23:41:40 | INFO | fairseq.trainer | layer 23 total_norm 1500.986083984375
2023-09-14 23:41:40 | INFO | train_inner | epoch 001:     19 / 32 loss=428.377, sample_size=60, ntokens=61, wps=8.7, ups=0.14, wpb=61, bsz=1, num_updates=19, lr=0.003, gnorm=37.456, clip=100, train_wall=7, wall=111
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 0 grad norm = 4532.6416015625
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 1 grad norm = 3356.90478515625
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2545.584228515625
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1739.0521240234375
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1385.5753173828125
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1290.65478515625
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1127.16455078125
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1067.2149658203125
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1039.2900390625
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 9 grad norm = 904.3703002929688
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 10 grad norm = 873.1713256835938
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 11 grad norm = 682.4135131835938
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 12 grad norm = 664.2709350585938
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 13 grad norm = 587.82080078125
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 14 grad norm = 643.1738891601562
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 15 grad norm = 695.406494140625
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 16 grad norm = 819.1282348632812
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 17 grad norm = 768.1509399414062
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 18 grad norm = 634.1506958007812
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 19 grad norm = 553.37353515625
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 20 grad norm = 544.0250854492188
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 21 grad norm = 568.9135131835938
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 22 grad norm = 691.10546875
2023-09-14 23:41:45 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1542.6796875
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 0 total_norm 4532.6416015625
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 1 total_norm 3356.90478515625
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 2 total_norm 2545.584228515625
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 3 total_norm 1739.0521240234375
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 4 total_norm 1385.5753173828125
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 5 total_norm 1290.65478515625
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 6 total_norm 1127.16455078125
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 7 total_norm 1067.2149658203125
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 8 total_norm 1039.2900390625
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 9 total_norm 904.3703002929688
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 10 total_norm 873.1713256835938
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 11 total_norm 682.4135131835938
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 12 total_norm 664.2709350585938
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 13 total_norm 587.82080078125
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 14 total_norm 643.1738891601562
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 15 total_norm 695.406494140625
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 16 total_norm 819.1282348632812
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 17 total_norm 768.1509399414062
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 18 total_norm 634.1506958007812
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 19 total_norm 553.37353515625
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 20 total_norm 544.0250854492188
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 21 total_norm 568.9135131835938
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 22 total_norm 691.10546875
2023-09-14 23:41:46 | INFO | fairseq.trainer | layer 23 total_norm 1542.6796875
2023-09-14 23:41:46 | INFO | train_inner | epoch 001:     20 / 32 loss=472.143, sample_size=41, ntokens=42, wps=6.7, ups=0.16, wpb=42, bsz=1, num_updates=20, lr=0.003, gnorm=41.974, clip=100, train_wall=6, wall=117
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 0 grad norm = 4614.365234375
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 1 grad norm = 3436.451171875
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2700.32421875
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1718.52099609375
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1358.15625
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1251.787353515625
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1100.712158203125
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1044.0252685546875
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1036.0675048828125
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 9 grad norm = 907.5309448242188
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 10 grad norm = 878.5824584960938
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 11 grad norm = 704.6144409179688
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 12 grad norm = 681.0357666015625
2023-09-14 23:41:51 | INFO | fairseq.trainer | decoder layer 13 grad norm = 605.5386352539062
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 14 grad norm = 667.7251586914062
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 15 grad norm = 716.876953125
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 16 grad norm = 802.1135864257812
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 17 grad norm = 760.9794921875
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 18 grad norm = 664.428466796875
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 19 grad norm = 574.9404907226562
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 20 grad norm = 560.7658081054688
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 21 grad norm = 587.4423828125
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 22 grad norm = 696.6806640625
2023-09-14 23:41:52 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1534.238525390625
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 0 total_norm 4614.365234375
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 1 total_norm 3436.451171875
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 2 total_norm 2700.32421875
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 3 total_norm 1718.52099609375
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 4 total_norm 1358.15625
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 5 total_norm 1251.787353515625
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 6 total_norm 1100.712158203125
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 7 total_norm 1044.0252685546875
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 8 total_norm 1036.0675048828125
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 9 total_norm 907.5309448242188
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 10 total_norm 878.5824584960938
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 11 total_norm 704.6144409179688
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 12 total_norm 681.0357666015625
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 13 total_norm 605.5386352539062
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 14 total_norm 667.7251586914062
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 15 total_norm 716.876953125
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 16 total_norm 802.1135864257812
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 17 total_norm 760.9794921875
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 18 total_norm 664.428466796875
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 19 total_norm 574.9404907226562
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 20 total_norm 560.7658081054688
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 21 total_norm 587.4423828125
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 22 total_norm 696.6806640625
2023-09-14 23:41:52 | INFO | fairseq.trainer | layer 23 total_norm 1534.238525390625
2023-09-14 23:41:52 | INFO | train_inner | epoch 001:     21 / 32 loss=428.901, sample_size=46, ntokens=47, wps=7.8, ups=0.17, wpb=47, bsz=1, num_updates=21, lr=0.003, gnorm=36.887, clip=100, train_wall=6, wall=123
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2288.0615234375
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 1 grad norm = 1865.0885009765625
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1587.9705810546875
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1160.4107666015625
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 4 grad norm = 971.5267333984375
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 5 grad norm = 895.7606811523438
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 6 grad norm = 912.4967041015625
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 7 grad norm = 937.5172729492188
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 8 grad norm = 977.3779907226562
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 9 grad norm = 877.0740356445312
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 10 grad norm = 859.9909057617188
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 11 grad norm = 716.9052734375
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 12 grad norm = 693.891845703125
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 13 grad norm = 619.2698364257812
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 14 grad norm = 681.7951049804688
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 15 grad norm = 731.7001342773438
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 16 grad norm = 784.2067260742188
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 17 grad norm = 741.6390380859375
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 18 grad norm = 680.3360595703125
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 19 grad norm = 583.0918579101562
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 20 grad norm = 571.8706665039062
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 21 grad norm = 596.0573120117188
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 22 grad norm = 710.72021484375
2023-09-14 23:41:56 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1537.830810546875
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 0 total_norm 2288.0615234375
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 1 total_norm 1865.0885009765625
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 2 total_norm 1587.9705810546875
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 3 total_norm 1160.4107666015625
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 4 total_norm 971.5267333984375
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 5 total_norm 895.7606811523438
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 6 total_norm 912.4967041015625
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 7 total_norm 937.5172729492188
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 8 total_norm 977.3779907226562
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 9 total_norm 877.0740356445312
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 10 total_norm 859.9909057617188
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 11 total_norm 716.9052734375
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 12 total_norm 693.891845703125
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 13 total_norm 619.2698364257812
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 14 total_norm 681.7951049804688
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 15 total_norm 731.7001342773438
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 16 total_norm 784.2067260742188
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 17 total_norm 741.6390380859375
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 18 total_norm 680.3360595703125
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 19 total_norm 583.0918579101562
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 20 total_norm 571.8706665039062
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 21 total_norm 596.0573120117188
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 22 total_norm 710.72021484375
2023-09-14 23:41:57 | INFO | fairseq.trainer | layer 23 total_norm 1537.830810546875
2023-09-14 23:41:57 | INFO | train_inner | epoch 001:     22 / 32 loss=377.35, sample_size=21, ntokens=22, wps=4.6, ups=0.21, wpb=22, bsz=1, num_updates=22, lr=0.003, gnorm=35.696, clip=100, train_wall=5, wall=128
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 0 grad norm = 5940.4443359375
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 1 grad norm = 4729.35986328125
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 2 grad norm = 3687.937255859375
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1933.5150146484375
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1466.2271728515625
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 5 grad norm = 1250.9178466796875
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 6 grad norm = 1099.775390625
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 7 grad norm = 1065.2783203125
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1085.5540771484375
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 9 grad norm = 932.0553588867188
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 10 grad norm = 891.0558471679688
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 11 grad norm = 749.0765380859375
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 12 grad norm = 728.129638671875
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 13 grad norm = 652.3778686523438
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 14 grad norm = 727.0301513671875
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 15 grad norm = 773.6187133789062
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 16 grad norm = 804.1878662109375
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 17 grad norm = 769.2545776367188
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 18 grad norm = 696.3385620117188
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 19 grad norm = 596.3013916015625
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 20 grad norm = 587.68994140625
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 21 grad norm = 606.6193237304688
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 22 grad norm = 709.9735717773438
2023-09-14 23:42:03 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1538.9930419921875
2023-09-14 23:42:03 | INFO | fairseq.trainer | layer 0 total_norm 5940.4443359375
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 1 total_norm 4729.35986328125
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 2 total_norm 3687.937255859375
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 3 total_norm 1933.5150146484375
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 4 total_norm 1466.2271728515625
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 5 total_norm 1250.9178466796875
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 6 total_norm 1099.775390625
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 7 total_norm 1065.2783203125
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 8 total_norm 1085.5540771484375
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 9 total_norm 932.0553588867188
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 10 total_norm 891.0558471679688
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 11 total_norm 749.0765380859375
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 12 total_norm 728.129638671875
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 13 total_norm 652.3778686523438
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 14 total_norm 727.0301513671875
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 15 total_norm 773.6187133789062
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 16 total_norm 804.1878662109375
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 17 total_norm 769.2545776367188
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 18 total_norm 696.3385620117188
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 19 total_norm 596.3013916015625
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 20 total_norm 587.68994140625
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 21 total_norm 606.6193237304688
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 22 total_norm 709.9735717773438
2023-09-14 23:42:04 | INFO | fairseq.trainer | layer 23 total_norm 1538.9930419921875
2023-09-14 23:42:04 | INFO | train_inner | epoch 001:     23 / 32 loss=403.678, sample_size=68, ntokens=69, wps=10, ups=0.14, wpb=69, bsz=1, num_updates=23, lr=0.003, gnorm=36.886, clip=100, train_wall=7, wall=135
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2092.658935546875
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 1 grad norm = 1732.4072265625
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1577.510986328125
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1111.04052734375
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 4 grad norm = 950.43994140625
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 5 grad norm = 885.6217651367188
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 6 grad norm = 912.6636352539062
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 7 grad norm = 933.6329956054688
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 8 grad norm = 979.025390625
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 9 grad norm = 878.63134765625
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 10 grad norm = 861.6812133789062
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 11 grad norm = 761.4727783203125
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 12 grad norm = 740.6807250976562
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 13 grad norm = 663.74169921875
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 14 grad norm = 741.998046875
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 15 grad norm = 786.4786987304688
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 16 grad norm = 780.1334228515625
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 17 grad norm = 740.6755981445312
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 18 grad norm = 680.0133666992188
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 19 grad norm = 602.604736328125
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 20 grad norm = 595.9644775390625
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 21 grad norm = 613.0767822265625
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 22 grad norm = 717.71240234375
2023-09-14 23:42:08 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1535.184326171875
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 0 total_norm 2092.658935546875
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 1 total_norm 1732.4072265625
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 2 total_norm 1577.510986328125
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 3 total_norm 1111.04052734375
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 4 total_norm 950.43994140625
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 5 total_norm 885.6217651367188
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 6 total_norm 912.6636352539062
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 7 total_norm 933.6329956054688
2023-09-14 23:42:08 | INFO | fairseq.trainer | layer 8 total_norm 979.025390625
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 9 total_norm 878.63134765625
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 10 total_norm 861.6812133789062
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 11 total_norm 761.4727783203125
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 12 total_norm 740.6807250976562
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 13 total_norm 663.74169921875
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 14 total_norm 741.998046875
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 15 total_norm 786.4786987304688
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 16 total_norm 780.1334228515625
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 17 total_norm 740.6755981445312
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 18 total_norm 680.0133666992188
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 19 total_norm 602.604736328125
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 20 total_norm 595.9644775390625
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 21 total_norm 613.0767822265625
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 22 total_norm 717.71240234375
2023-09-14 23:42:09 | INFO | fairseq.trainer | layer 23 total_norm 1535.184326171875
2023-09-14 23:42:09 | INFO | train_inner | epoch 001:     24 / 32 loss=394.551, sample_size=18, ntokens=19, wps=3.8, ups=0.2, wpb=19, bsz=1, num_updates=24, lr=0.003, gnorm=38.48, clip=100, train_wall=5, wall=140
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3299.19482421875
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2499.5927734375
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2165.212890625
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1317.24853515625
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 4 grad norm = 1072.4493408203125
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 5 grad norm = 974.6616821289062
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 6 grad norm = 958.7974853515625
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 7 grad norm = 967.3889770507812
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1005.6681518554688
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 9 grad norm = 891.8204956054688
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 10 grad norm = 862.76171875
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 11 grad norm = 775.514404296875
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 12 grad norm = 755.217529296875
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 13 grad norm = 677.7005004882812
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 14 grad norm = 760.0972900390625
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 15 grad norm = 801.1858520507812
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 16 grad norm = 787.6859130859375
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 17 grad norm = 744.9746704101562
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 18 grad norm = 684.197265625
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 19 grad norm = 609.3078002929688
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 20 grad norm = 610.2002563476562
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 21 grad norm = 618.2119140625
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 22 grad norm = 720.7891235351562
2023-09-14 23:42:14 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1540.4652099609375
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 0 total_norm 3299.19482421875
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 1 total_norm 2499.5927734375
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 2 total_norm 2165.212890625
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 3 total_norm 1317.24853515625
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 4 total_norm 1072.4493408203125
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 5 total_norm 974.6616821289062
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 6 total_norm 958.7974853515625
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 7 total_norm 967.3889770507812
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 8 total_norm 1005.6681518554688
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 9 total_norm 891.8204956054688
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 10 total_norm 862.76171875
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 11 total_norm 775.514404296875
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 12 total_norm 755.217529296875
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 13 total_norm 677.7005004882812
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 14 total_norm 760.0972900390625
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 15 total_norm 801.1858520507812
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 16 total_norm 787.6859130859375
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 17 total_norm 744.9746704101562
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 18 total_norm 684.197265625
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 19 total_norm 609.3078002929688
2023-09-14 23:42:14 | INFO | fairseq.trainer | layer 20 total_norm 610.2002563476562
2023-09-14 23:42:15 | INFO | fairseq.trainer | layer 21 total_norm 618.2119140625
2023-09-14 23:42:15 | INFO | fairseq.trainer | layer 22 total_norm 720.7891235351562
2023-09-14 23:42:15 | INFO | fairseq.trainer | layer 23 total_norm 1540.4652099609375
2023-09-14 23:42:15 | INFO | train_inner | epoch 001:     25 / 32 loss=425.166, sample_size=32, ntokens=33, wps=5.9, ups=0.18, wpb=33, bsz=1, num_updates=25, lr=0.003, gnorm=37.496, clip=100, train_wall=6, wall=146
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2713.408447265625
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2108.921630859375
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1945.085693359375
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1186.529541015625
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 4 grad norm = 989.3779907226562
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 5 grad norm = 887.075439453125
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 6 grad norm = 919.7572631835938
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 7 grad norm = 951.2892456054688
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1006.5657958984375
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 9 grad norm = 894.308837890625
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 10 grad norm = 864.4011840820312
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 11 grad norm = 789.11767578125
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 12 grad norm = 764.9263305664062
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 13 grad norm = 686.6499633789062
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 14 grad norm = 765.4407958984375
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 15 grad norm = 793.1035766601562
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 16 grad norm = 756.4312744140625
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 17 grad norm = 722.1787109375
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 18 grad norm = 677.072265625
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 19 grad norm = 618.661865234375
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 20 grad norm = 616.6845092773438
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 21 grad norm = 612.3873901367188
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 22 grad norm = 697.7958374023438
2023-09-14 23:42:19 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1525.3299560546875
2023-09-14 23:42:19 | INFO | fairseq.trainer | layer 0 total_norm 2713.408447265625
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 1 total_norm 2108.921630859375
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 2 total_norm 1945.085693359375
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 3 total_norm 1186.529541015625
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 4 total_norm 989.3779907226562
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 5 total_norm 887.075439453125
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 6 total_norm 919.7572631835938
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 7 total_norm 951.2892456054688
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 8 total_norm 1006.5657958984375
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 9 total_norm 894.308837890625
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 10 total_norm 864.4011840820312
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 11 total_norm 789.11767578125
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 12 total_norm 764.9263305664062
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 13 total_norm 686.6499633789062
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 14 total_norm 765.4407958984375
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 15 total_norm 793.1035766601562
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 16 total_norm 756.4312744140625
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 17 total_norm 722.1787109375
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 18 total_norm 677.072265625
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 19 total_norm 618.661865234375
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 20 total_norm 616.6845092773438
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 21 total_norm 612.3873901367188
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 22 total_norm 697.7958374023438
2023-09-14 23:42:20 | INFO | fairseq.trainer | layer 23 total_norm 1525.3299560546875
2023-09-14 23:42:20 | INFO | train_inner | epoch 001:     26 / 32 loss=394.594, sample_size=26, ntokens=27, wps=4.7, ups=0.17, wpb=27, bsz=1, num_updates=26, lr=0.003, gnorm=36.94, clip=100, train_wall=6, wall=151
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2738.408447265625
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2084.146484375
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1976.8756103515625
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1177.3001708984375
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 4 grad norm = 961.8511962890625
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 5 grad norm = 855.4859619140625
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 6 grad norm = 895.5560302734375
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 7 grad norm = 931.4259033203125
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 8 grad norm = 986.884765625
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 9 grad norm = 882.3934936523438
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 10 grad norm = 857.0853881835938
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 11 grad norm = 798.8982543945312
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 12 grad norm = 774.8021240234375
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 13 grad norm = 697.312744140625
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 14 grad norm = 778.0743408203125
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 15 grad norm = 800.0275268554688
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 16 grad norm = 766.1618041992188
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 17 grad norm = 735.7583618164062
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 18 grad norm = 679.05712890625
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 19 grad norm = 628.7330932617188
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 20 grad norm = 629.6423950195312
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 21 grad norm = 616.639404296875
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 22 grad norm = 691.870361328125
2023-09-14 23:42:25 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1535.023681640625
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 0 total_norm 2738.408447265625
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 1 total_norm 2084.146484375
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 2 total_norm 1976.8756103515625
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 3 total_norm 1177.3001708984375
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 4 total_norm 961.8511962890625
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 5 total_norm 855.4859619140625
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 6 total_norm 895.5560302734375
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 7 total_norm 931.4259033203125
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 8 total_norm 986.884765625
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 9 total_norm 882.3934936523438
2023-09-14 23:42:25 | INFO | fairseq.trainer | layer 10 total_norm 857.0853881835938
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 11 total_norm 798.8982543945312
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 12 total_norm 774.8021240234375
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 13 total_norm 697.312744140625
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 14 total_norm 778.0743408203125
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 15 total_norm 800.0275268554688
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 16 total_norm 766.1618041992188
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 17 total_norm 735.7583618164062
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 18 total_norm 679.05712890625
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 19 total_norm 628.7330932617188
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 20 total_norm 629.6423950195312
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 21 total_norm 616.639404296875
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 22 total_norm 691.870361328125
2023-09-14 23:42:26 | INFO | fairseq.trainer | layer 23 total_norm 1535.023681640625
2023-09-14 23:42:26 | INFO | train_inner | epoch 001:     27 / 32 loss=418.002, sample_size=25, ntokens=26, wps=4.8, ups=0.18, wpb=26, bsz=1, num_updates=27, lr=0.003, gnorm=38.838, clip=100, train_wall=5, wall=157
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2759.638671875
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2102.418212890625
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2020.730712890625
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1133.2972412109375
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 4 grad norm = 910.9124145507812
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 5 grad norm = 812.8277587890625
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 6 grad norm = 889.1276245117188
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 7 grad norm = 943.3973999023438
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1003.81982421875
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 9 grad norm = 889.6837768554688
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 10 grad norm = 862.0317993164062
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 11 grad norm = 810.2932739257812
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 12 grad norm = 782.9677734375
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 13 grad norm = 704.1299438476562
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 14 grad norm = 779.26904296875
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 15 grad norm = 791.0031127929688
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 16 grad norm = 754.7493896484375
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 17 grad norm = 713.486328125
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 18 grad norm = 670.5468139648438
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 19 grad norm = 632.103759765625
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 20 grad norm = 631.2142333984375
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 21 grad norm = 602.658447265625
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 22 grad norm = 660.7782592773438
2023-09-14 23:42:30 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1527.1414794921875
2023-09-14 23:42:30 | INFO | fairseq.trainer | layer 0 total_norm 2759.638671875
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 1 total_norm 2102.418212890625
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 2 total_norm 2020.730712890625
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 3 total_norm 1133.2972412109375
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 4 total_norm 910.9124145507812
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 5 total_norm 812.8277587890625
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 6 total_norm 889.1276245117188
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 7 total_norm 943.3973999023438
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 8 total_norm 1003.81982421875
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 9 total_norm 889.6837768554688
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 10 total_norm 862.0317993164062
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 11 total_norm 810.2932739257812
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 12 total_norm 782.9677734375
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 13 total_norm 704.1299438476562
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 14 total_norm 779.26904296875
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 15 total_norm 791.0031127929688
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 16 total_norm 754.7493896484375
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 17 total_norm 713.486328125
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 18 total_norm 670.5468139648438
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 19 total_norm 632.103759765625
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 20 total_norm 631.2142333984375
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 21 total_norm 602.658447265625
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 22 total_norm 660.7782592773438
2023-09-14 23:42:31 | INFO | fairseq.trainer | layer 23 total_norm 1527.1414794921875
2023-09-14 23:42:31 | INFO | train_inner | epoch 001:     28 / 32 loss=398.243, sample_size=27, ntokens=28, wps=5.3, ups=0.19, wpb=28, bsz=1, num_updates=28, lr=0.003, gnorm=36.901, clip=100, train_wall=5, wall=162
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3553.145751953125
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2677.611572265625
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2464.437255859375
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1231.7315673828125
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 4 grad norm = 971.8833618164062
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 5 grad norm = 862.2894897460938
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 6 grad norm = 924.0322265625
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 7 grad norm = 976.901123046875
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1042.00634765625
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 9 grad norm = 911.4215087890625
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 10 grad norm = 874.2407836914062
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 11 grad norm = 828.0776977539062
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 12 grad norm = 793.22900390625
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 13 grad norm = 711.2976684570312
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 14 grad norm = 778.8460693359375
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 15 grad norm = 787.30908203125
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 16 grad norm = 738.3478393554688
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 17 grad norm = 702.8961791992188
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 18 grad norm = 668.5313110351562
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 19 grad norm = 634.5401611328125
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 20 grad norm = 628.6631469726562
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 21 grad norm = 584.8030395507812
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 22 grad norm = 630.2976684570312
2023-09-14 23:42:36 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1542.079345703125
2023-09-14 23:42:36 | INFO | fairseq.trainer | layer 0 total_norm 3553.145751953125
2023-09-14 23:42:36 | INFO | fairseq.trainer | layer 1 total_norm 2677.611572265625
2023-09-14 23:42:36 | INFO | fairseq.trainer | layer 2 total_norm 2464.437255859375
2023-09-14 23:42:36 | INFO | fairseq.trainer | layer 3 total_norm 1231.7315673828125
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 4 total_norm 971.8833618164062
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 5 total_norm 862.2894897460938
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 6 total_norm 924.0322265625
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 7 total_norm 976.901123046875
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 8 total_norm 1042.00634765625
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 9 total_norm 911.4215087890625
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 10 total_norm 874.2407836914062
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 11 total_norm 828.0776977539062
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 12 total_norm 793.22900390625
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 13 total_norm 711.2976684570312
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 14 total_norm 778.8460693359375
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 15 total_norm 787.30908203125
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 16 total_norm 738.3478393554688
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 17 total_norm 702.8961791992188
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 18 total_norm 668.5313110351562
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 19 total_norm 634.5401611328125
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 20 total_norm 628.6631469726562
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 21 total_norm 584.8030395507812
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 22 total_norm 630.2976684570312
2023-09-14 23:42:37 | INFO | fairseq.trainer | layer 23 total_norm 1542.079345703125
2023-09-14 23:42:37 | INFO | train_inner | epoch 001:     29 / 32 loss=402.291, sample_size=38, ntokens=39, wps=6.6, ups=0.17, wpb=39, bsz=1, num_updates=29, lr=0.003, gnorm=35.582, clip=100, train_wall=6, wall=168
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 0 grad norm = 3391.53515625
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2490.30615234375
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2509.998291015625
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1206.5028076171875
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 4 grad norm = 950.6162719726562
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 5 grad norm = 831.7164306640625
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 6 grad norm = 905.2095947265625
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 7 grad norm = 957.3060913085938
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1013.107666015625
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 9 grad norm = 895.58203125
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 10 grad norm = 867.1190185546875
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 11 grad norm = 841.3938598632812
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 12 grad norm = 802.0631103515625
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 13 grad norm = 718.8204345703125
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 14 grad norm = 781.0418701171875
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 15 grad norm = 788.43212890625
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 16 grad norm = 732.4578857421875
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 17 grad norm = 700.9691772460938
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 18 grad norm = 671.142822265625
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 19 grad norm = 637.2960205078125
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 20 grad norm = 626.6837768554688
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 21 grad norm = 574.4154663085938
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 22 grad norm = 623.1029663085938
2023-09-14 23:42:42 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1556.877197265625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 0 total_norm 3391.53515625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 1 total_norm 2490.30615234375
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 2 total_norm 2509.998291015625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 3 total_norm 1206.5028076171875
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 4 total_norm 950.6162719726562
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 5 total_norm 831.7164306640625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 6 total_norm 905.2095947265625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 7 total_norm 957.3060913085938
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 8 total_norm 1013.107666015625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 9 total_norm 895.58203125
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 10 total_norm 867.1190185546875
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 11 total_norm 841.3938598632812
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 12 total_norm 802.0631103515625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 13 total_norm 718.8204345703125
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 14 total_norm 781.0418701171875
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 15 total_norm 788.43212890625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 16 total_norm 732.4578857421875
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 17 total_norm 700.9691772460938
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 18 total_norm 671.142822265625
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 19 total_norm 637.2960205078125
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 20 total_norm 626.6837768554688
2023-09-14 23:42:42 | INFO | fairseq.trainer | layer 21 total_norm 574.4154663085938
2023-09-14 23:42:43 | INFO | fairseq.trainer | layer 22 total_norm 623.1029663085938
2023-09-14 23:42:43 | INFO | fairseq.trainer | layer 23 total_norm 1556.877197265625
2023-09-14 23:42:43 | INFO | train_inner | epoch 001:     30 / 32 loss=435.685, sample_size=32, ntokens=33, wps=5.9, ups=0.18, wpb=33, bsz=1, num_updates=30, lr=0.003, gnorm=41.054, clip=100, train_wall=6, wall=174
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2709.75830078125
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 1 grad norm = 2017.10693359375
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 2 grad norm = 2090.234375
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 3 grad norm = 1095.557861328125
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 4 grad norm = 888.4844360351562
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 5 grad norm = 799.7772827148438
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 6 grad norm = 891.1508178710938
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 7 grad norm = 942.1527099609375
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 8 grad norm = 999.036376953125
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 9 grad norm = 889.5018920898438
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 10 grad norm = 863.6448364257812
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 11 grad norm = 852.6021728515625
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 12 grad norm = 809.2809448242188
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 13 grad norm = 724.379150390625
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 14 grad norm = 782.439453125
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 15 grad norm = 788.6106567382812
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 16 grad norm = 727.61767578125
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 17 grad norm = 697.569091796875
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 18 grad norm = 670.1443481445312
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 19 grad norm = 639.3023071289062
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 20 grad norm = 626.548828125
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 21 grad norm = 567.3186645507812
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 22 grad norm = 621.4262084960938
2023-09-14 23:42:47 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1544.4189453125
2023-09-14 23:42:47 | INFO | fairseq.trainer | layer 0 total_norm 2709.75830078125
2023-09-14 23:42:47 | INFO | fairseq.trainer | layer 1 total_norm 2017.10693359375
2023-09-14 23:42:47 | INFO | fairseq.trainer | layer 2 total_norm 2090.234375
2023-09-14 23:42:47 | INFO | fairseq.trainer | layer 3 total_norm 1095.557861328125
2023-09-14 23:42:47 | INFO | fairseq.trainer | layer 4 total_norm 888.4844360351562
2023-09-14 23:42:47 | INFO | fairseq.trainer | layer 5 total_norm 799.7772827148438
2023-09-14 23:42:47 | INFO | fairseq.trainer | layer 6 total_norm 891.1508178710938
2023-09-14 23:42:47 | INFO | fairseq.trainer | layer 7 total_norm 942.1527099609375
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 8 total_norm 999.036376953125
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 9 total_norm 889.5018920898438
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 10 total_norm 863.6448364257812
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 11 total_norm 852.6021728515625
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 12 total_norm 809.2809448242188
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 13 total_norm 724.379150390625
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 14 total_norm 782.439453125
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 15 total_norm 788.6106567382812
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 16 total_norm 727.61767578125
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 17 total_norm 697.569091796875
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 18 total_norm 670.1443481445312
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 19 total_norm 639.3023071289062
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 20 total_norm 626.548828125
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 21 total_norm 567.3186645507812
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 22 total_norm 621.4262084960938
2023-09-14 23:42:48 | INFO | fairseq.trainer | layer 23 total_norm 1544.4189453125
2023-09-14 23:42:48 | INFO | train_inner | epoch 001:     31 / 32 loss=419.63, sample_size=25, ntokens=26, wps=4.8, ups=0.18, wpb=26, bsz=1, num_updates=31, lr=0.003, gnorm=39.002, clip=100, train_wall=5, wall=179
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 0 grad norm = 2379.931396484375
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 1 grad norm = 1843.706298828125
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 2 grad norm = 1673.1358642578125
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 3 grad norm = 982.7485961914062
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 4 grad norm = 805.5012817382812
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 5 grad norm = 760.2335815429688
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 6 grad norm = 878.6851806640625
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 7 grad norm = 938.6834106445312
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 8 grad norm = 1002.8072509765625
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 9 grad norm = 892.2601928710938
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 10 grad norm = 865.0746459960938
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 11 grad norm = 865.5519409179688
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 12 grad norm = 816.900634765625
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 13 grad norm = 730.9549560546875
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 14 grad norm = 783.9574584960938
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 15 grad norm = 787.871337890625
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 16 grad norm = 720.4051513671875
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 17 grad norm = 691.4967651367188
2023-09-14 23:42:52 | INFO | fairseq.trainer | decoder layer 18 grad norm = 670.5828857421875
2023-09-14 23:42:53 | INFO | fairseq.trainer | decoder layer 19 grad norm = 642.2579345703125
2023-09-14 23:42:53 | INFO | fairseq.trainer | decoder layer 20 grad norm = 625.4490356445312
2023-09-14 23:42:53 | INFO | fairseq.trainer | decoder layer 21 grad norm = 563.4326782226562
2023-09-14 23:42:53 | INFO | fairseq.trainer | decoder layer 22 grad norm = 629.7346801757812
2023-09-14 23:42:53 | INFO | fairseq.trainer | decoder layer 23 grad norm = 1534.797607421875
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 0 total_norm 2379.931396484375
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 1 total_norm 1843.706298828125
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 2 total_norm 1673.1358642578125
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 3 total_norm 982.7485961914062
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 4 total_norm 805.5012817382812
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 5 total_norm 760.2335815429688
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 6 total_norm 878.6851806640625
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 7 total_norm 938.6834106445312
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 8 total_norm 1002.8072509765625
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 9 total_norm 892.2601928710938
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 10 total_norm 865.0746459960938
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 11 total_norm 865.5519409179688
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 12 total_norm 816.900634765625
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 13 total_norm 730.9549560546875
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 14 total_norm 783.9574584960938
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 15 total_norm 787.871337890625
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 16 total_norm 720.4051513671875
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 17 total_norm 691.4967651367188
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 18 total_norm 670.5828857421875
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 19 total_norm 642.2579345703125
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 20 total_norm 625.4490356445312
2023-09-14 23:42:53 | INFO | fairseq.trainer | layer 21 total_norm 563.4326782226562
2023-09-14 23:42:54 | INFO | fairseq.trainer | layer 22 total_norm 629.7346801757812
2023-09-14 23:42:54 | INFO | fairseq.trainer | layer 23 total_norm 1534.797607421875
2023-09-14 23:42:54 | INFO | train_inner | epoch 001:     32 / 32 loss=382.412, sample_size=25, ntokens=26, wps=4.7, ups=0.18, wpb=26, bsz=1, num_updates=32, lr=0.003, gnorm=30.561, clip=100, train_wall=6, wall=185
2023-09-14 23:42:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 32 updates
2023-09-14 23:42:54 | INFO | fairseq.trainer | Saving checkpoint to /vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint1.pt
2023-09-14 23:43:43 | INFO | fairseq.trainer | Finished saving checkpoint to /vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint1.pt
2023-09-14 23:45:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint1.pt (epoch 1 @ 32 updates, score None) (writing took 156.45351107488386 seconds)
2023-09-14 23:45:30 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-09-14 23:45:30 | INFO | train | epoch 001 | loss 432.591 | sample_size 33.844 | ntokens 34.844 | wps 3.3 | ups 0.09 | wpb 34.8 | bsz 1 | num_updates 32 | lr 0.003 | gnorm 37.088 | clip 100 | train_wall 184 | wall 341
2023-09-14 23:45:30 | INFO | fairseq_cli.train | done training in 341.2 seconds
output_dir
2023-09-14 23:45:58 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-09-14 23:46:01 | INFO | fairseq_cli.train | {'task': {'_name': 'fs_eval', 'data': '-', 'seed': 4, 'eval_data': 'subj', 'test_split': 'test', 'required_batch_size_multiple': 1, 'gpt2_encoder_json': 'base_dir/gpt_icl/encoder.json', 'gpt2_vocab_bpe': 'base_dir/gpt_icl/vocab.bpe', 'gpt_dict': 'base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', 'ana_attn': 1, 'ana_rlt_dir': 'base_dir/ana_rlt/en_dense_lm_1_3b/subj', 'ana_setting': 'ftzs', 'optim_group': 'all', 'tokens_per_sample': 2048, 'max_target_positions': None, 'k': 0, 'temp_index': 0, 'permut_index': 0}, 'model': {'_name': 'gptmodel_large', 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 2048, 'decoder_output_dim': 2048, 'decoder_input_dim': 2048, 'decoder_ffn_embed_dim': 8192, 'decoder_layers': 24, 'decoder_attention_heads': 32, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 2048, 'max_target_positions': None, 'tpu': False, 'gpt_model_path': 'base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint_last.pt', 'use_linearization': '0', 'sum_extra_jvp_result': True}, 'criterion': {'_name': 'fs_eval', 'is_generation': False, 'beam': 3}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'common': {'no_progress_bar': False, 'log_interval': 1, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'distributed_world_size': 1, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': True, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'beam': 3, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'ema': {'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'tokenizer': None, 'bpe': None, 'optimizer': Namespace(no_progress_bar=False, log_interval=1, log_format='simple', log_file=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=4, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', tokenizer=None, bpe=None, optimizer='sgd', lr_scheduler='fixed', criterion='fs_eval', task='fs_eval', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=2, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid='2', max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=True, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='gptmodel_large', max_epoch=1, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.25], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=True, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='-', eval_data='subj', test_split='test', gpt2_encoder_json='base_dir/gpt_icl/encoder.json', gpt2_vocab_bpe='base_dir/gpt_icl/vocab.bpe', gpt_dict='base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', ana_attn=1, ana_rlt_dir='base_dir/ana_rlt/en_dense_lm_1_3b/subj', ana_setting='ftzs', optim_group='all', tokens_per_sample=2048, max_target_positions=None, k=0, temp_index=0, permut_index=0, momentum=0.0, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, is_generation=False, beam=3, checkpoint_activations=True, gpt_model_path='base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint_last.pt', use_linearization='0', no_seed_provided=False, decoder_layers=24, decoder_embed_dim=2048, decoder_attention_heads=32, decoder_learned_pos=False, offload_activations=False, decoder_input_dim=2048, decoder_output_dim=2048, decoder_ffn_embed_dim=8192, dropout=0.0, attention_dropout=0.0, activation_fn='gelu', share_decoder_input_output_embed=True, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, decoder_layerdrop=0, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, base_layers=0, base_sublayers=1, base_shuffle=False, add_bos_token=False, no_token_positional_embeddings=False, character_embeddings=False, decoder_normalize_before=True, no_decoder_final_norm=False, adaptive_input=False, adaptive_input_factor=4, adaptive_input_cutoff=None, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=False, layernorm_embedding=False, scale_fc=False, scale_attn=False, scale_heads=False, scale_resids=False, _name='sgd')}
2023-09-14 23:46:01 | INFO | struprompting.tasks.fs_eval | dictionary: 51200 types
2023-09-14 23:46:03 | WARNING | datasets.builder | Using custom data configuration SetFit--subj-693a635c625bebac
2023-09-14 23:46:03 | WARNING | datasets.builder | Reusing dataset json (/a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 373.84it/s]
2023-09-14 23:46:03 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-41679576bb6cfc7c.arrow
2023-09-14 23:46:03 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-6b58bb8b23415efd.arrow
/vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/struprompting/tasks/fewshot_task.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  return np.array(src_tokens), np.array(gpt_loss_mask), np.array(labels), max_len
2023-09-14 23:47:32 | WARNING | fairseq.models.fairseq_model | using 'args' is deprecated, please update your code to use dataclass config
2023-09-14 23:47:33 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): GPTDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 2048, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=2048, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=2048, bias=True)
        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=2048, out_features=51200, bias=False)
  )
)
2023-09-14 23:47:33 | INFO | fairseq_cli.train | task: FewshotEval
2023-09-14 23:47:33 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2023-09-14 23:47:33 | INFO | fairseq_cli.train | criterion: FewshotEvalCriterion
2023-09-14 23:47:33 | INFO | fairseq_cli.train | num. shared model params: 1,313,460,224 (num. trained: 2,048)
2023-09-14 23:47:33 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-14 23:47:33 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-09-14 23:47:33 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-09-14 23:47:33 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 2
2023-09-14 23:47:33 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt
2023-09-14 23:47:33 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt
2023-09-14 23:47:33 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-14 23:47:33 | INFO | fairseq_cli.train | Start validating
2023-09-14 23:47:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
------------ example 0 input str valid is ['Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:', 'Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:']
------------ example 0 label str valid is [' objective', ' subjective']
------------ example 1 input str valid is ['Input: if the story lacks bite , the performances are never less than affectionate . Type:', 'Input: if the story lacks bite , the performances are never less than affectionate . Type:']
------------ example 1 label str valid is [' objective', ' subjective']
NOTE: true K of baseline is 0, max valid len is 174
| Loaded valid with 4000 samples
| Loaded train with 4000 samples
2023-09-15 00:07:28 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.178 | nll_loss 0.203 | accuracy 51.7 | f1 0 | pos_proportion 51.7 | neg_proportion 48.3 | wps 116.7 | wpb 69.7 | bsz 1 | num_updates 0
output_dir
2023-09-15 00:09:10 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-09-15 00:09:13 | INFO | fairseq_cli.train | {'task': {'_name': 'fs_eval', 'data': '-', 'seed': 4, 'eval_data': 'subj', 'test_split': 'test', 'required_batch_size_multiple': 1, 'gpt2_encoder_json': 'base_dir/gpt_icl/encoder.json', 'gpt2_vocab_bpe': 'base_dir/gpt_icl/vocab.bpe', 'gpt_dict': 'base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', 'ana_attn': 1, 'ana_rlt_dir': 'base_dir/ana_rlt/en_dense_lm_1_3b/subj', 'ana_setting': 'zs', 'optim_group': 'all', 'tokens_per_sample': 2048, 'max_target_positions': None, 'k': 0, 'temp_index': 0, 'permut_index': 0}, 'model': {'_name': 'gptmodel_large', 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 2048, 'decoder_output_dim': 2048, 'decoder_input_dim': 2048, 'decoder_ffn_embed_dim': 8192, 'decoder_layers': 24, 'decoder_attention_heads': 32, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 2048, 'max_target_positions': None, 'tpu': False, 'gpt_model_path': 'base_dir/gpt_icl/en_dense_lm_1_3b/model.pt', 'use_linearization': '0', 'sum_extra_jvp_result': True}, 'criterion': {'_name': 'fs_eval', 'is_generation': False, 'beam': 3}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'common': {'no_progress_bar': False, 'log_interval': 1, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'distributed_world_size': 1, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': True, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'beam': 3, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'ema': {'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'tokenizer': None, 'bpe': None, 'optimizer': Namespace(no_progress_bar=False, log_interval=1, log_format='simple', log_file=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=4, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', tokenizer=None, bpe=None, optimizer='sgd', lr_scheduler='fixed', criterion='fs_eval', task='fs_eval', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=2, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid='2', max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=True, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='gptmodel_large', max_epoch=1, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.25], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=True, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='-', eval_data='subj', test_split='test', gpt2_encoder_json='base_dir/gpt_icl/encoder.json', gpt2_vocab_bpe='base_dir/gpt_icl/vocab.bpe', gpt_dict='base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', ana_attn=1, ana_rlt_dir='base_dir/ana_rlt/en_dense_lm_1_3b/subj', ana_setting='zs', optim_group='all', tokens_per_sample=2048, max_target_positions=None, k=0, temp_index=0, permut_index=0, momentum=0.0, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, is_generation=False, beam=3, checkpoint_activations=True, gpt_model_path='base_dir/gpt_icl/en_dense_lm_1_3b/model.pt', use_linearization='0', no_seed_provided=False, decoder_layers=24, decoder_embed_dim=2048, decoder_attention_heads=32, decoder_learned_pos=False, offload_activations=False, decoder_input_dim=2048, decoder_output_dim=2048, decoder_ffn_embed_dim=8192, dropout=0.0, attention_dropout=0.0, activation_fn='gelu', share_decoder_input_output_embed=True, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, decoder_layerdrop=0, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, base_layers=0, base_sublayers=1, base_shuffle=False, add_bos_token=False, no_token_positional_embeddings=False, character_embeddings=False, decoder_normalize_before=True, no_decoder_final_norm=False, adaptive_input=False, adaptive_input_factor=4, adaptive_input_cutoff=None, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=False, layernorm_embedding=False, scale_fc=False, scale_attn=False, scale_heads=False, scale_resids=False, _name='sgd')}
2023-09-15 00:09:13 | INFO | struprompting.tasks.fs_eval | dictionary: 51200 types
2023-09-15 00:09:15 | WARNING | datasets.builder | Using custom data configuration SetFit--subj-693a635c625bebac
2023-09-15 00:09:15 | WARNING | datasets.builder | Reusing dataset json (/a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 401.89it/s]
2023-09-15 00:09:15 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-41679576bb6cfc7c.arrow
2023-09-15 00:09:15 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-6b58bb8b23415efd.arrow
/vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/struprompting/tasks/fewshot_task.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  return np.array(src_tokens), np.array(gpt_loss_mask), np.array(labels), max_len
2023-09-15 00:09:40 | WARNING | fairseq.models.fairseq_model | using 'args' is deprecated, please update your code to use dataclass config
2023-09-15 00:09:41 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): GPTDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 2048, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=2048, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=2048, bias=True)
        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=2048, out_features=51200, bias=False)
  )
)
2023-09-15 00:09:41 | INFO | fairseq_cli.train | task: FewshotEval
2023-09-15 00:09:41 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2023-09-15 00:09:41 | INFO | fairseq_cli.train | criterion: FewshotEvalCriterion
2023-09-15 00:09:41 | INFO | fairseq_cli.train | num. shared model params: 1,313,460,224 (num. trained: 2,048)
2023-09-15 00:09:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-15 00:09:41 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-09-15 00:09:41 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-09-15 00:09:41 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 2
2023-09-15 00:09:41 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt
2023-09-15 00:09:41 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt
2023-09-15 00:09:41 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-15 00:09:41 | INFO | fairseq_cli.train | Start validating
2023-09-15 00:09:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
------------ example 0 input str valid is ['Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:', 'Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:']
------------ example 0 label str valid is [' objective', ' subjective']
------------ example 1 input str valid is ['Input: if the story lacks bite , the performances are never less than affectionate . Type:', 'Input: if the story lacks bite , the performances are never less than affectionate . Type:']
------------ example 1 label str valid is [' objective', ' subjective']
NOTE: true K of baseline is 0, max valid len is 174
| Loaded valid with 4000 samples
| Loaded train with 4000 samples
2023-09-15 00:27:58 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.537 | nll_loss 0.209 | accuracy 72.4 | f1 0 | pos_proportion 51.7 | neg_proportion 48.3 | wps 127.1 | wpb 69.7 | bsz 1 | num_updates 0
output_dir
2023-09-15 00:29:05 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-09-15 00:29:08 | INFO | fairseq_cli.train | {'task': {'_name': 'fs_eval', 'data': '-', 'seed': 4, 'eval_data': 'subj', 'test_split': 'test', 'required_batch_size_multiple': 1, 'gpt2_encoder_json': 'base_dir/gpt_icl/encoder.json', 'gpt2_vocab_bpe': 'base_dir/gpt_icl/vocab.bpe', 'gpt_dict': 'base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', 'ana_attn': 1, 'ana_rlt_dir': 'base_dir/ana_rlt/en_dense_lm_1_3b/subj', 'ana_setting': 'icl', 'optim_group': 'all', 'tokens_per_sample': 2048, 'max_target_positions': None, 'k': 32, 'temp_index': 0, 'permut_index': 0}, 'model': {'_name': 'gptmodel_large', 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 2048, 'decoder_output_dim': 2048, 'decoder_input_dim': 2048, 'decoder_ffn_embed_dim': 8192, 'decoder_layers': 24, 'decoder_attention_heads': 32, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 2048, 'max_target_positions': None, 'tpu': False, 'gpt_model_path': 'base_dir/gpt_icl/en_dense_lm_1_3b/model.pt', 'use_linearization': '0', 'sum_extra_jvp_result': True}, 'criterion': {'_name': 'fs_eval', 'is_generation': False, 'beam': 3}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'common': {'no_progress_bar': False, 'log_interval': 1, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'distributed_world_size': 1, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': True, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'beam': 3, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'ema': {'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'tokenizer': None, 'bpe': None, 'optimizer': Namespace(no_progress_bar=False, log_interval=1, log_format='simple', log_file=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=4, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', tokenizer=None, bpe=None, optimizer='sgd', lr_scheduler='fixed', criterion='fs_eval', task='fs_eval', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=2, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid='2', max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=True, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='gptmodel_large', max_epoch=1, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.25], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=True, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='-', eval_data='subj', test_split='test', gpt2_encoder_json='base_dir/gpt_icl/encoder.json', gpt2_vocab_bpe='base_dir/gpt_icl/vocab.bpe', gpt_dict='base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', ana_attn=1, ana_rlt_dir='base_dir/ana_rlt/en_dense_lm_1_3b/subj', ana_setting='icl', optim_group='all', tokens_per_sample=2048, max_target_positions=None, k=32, temp_index=0, permut_index=0, momentum=0.0, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, is_generation=False, beam=3, checkpoint_activations=True, gpt_model_path='base_dir/gpt_icl/en_dense_lm_1_3b/model.pt', use_linearization='0', no_seed_provided=False, decoder_layers=24, decoder_embed_dim=2048, decoder_attention_heads=32, decoder_learned_pos=False, offload_activations=False, decoder_input_dim=2048, decoder_output_dim=2048, decoder_ffn_embed_dim=8192, dropout=0.0, attention_dropout=0.0, activation_fn='gelu', share_decoder_input_output_embed=True, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, decoder_layerdrop=0, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, base_layers=0, base_sublayers=1, base_shuffle=False, add_bos_token=False, no_token_positional_embeddings=False, character_embeddings=False, decoder_normalize_before=True, no_decoder_final_norm=False, adaptive_input=False, adaptive_input_factor=4, adaptive_input_cutoff=None, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=False, layernorm_embedding=False, scale_fc=False, scale_attn=False, scale_heads=False, scale_resids=False, _name='sgd')}
2023-09-15 00:29:08 | INFO | struprompting.tasks.fs_eval | dictionary: 51200 types
2023-09-15 00:29:10 | WARNING | datasets.builder | Using custom data configuration SetFit--subj-693a635c625bebac
2023-09-15 00:29:10 | WARNING | datasets.builder | Reusing dataset json (/a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 411.00it/s]
2023-09-15 00:29:10 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-41679576bb6cfc7c.arrow
2023-09-15 00:29:10 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-6b58bb8b23415efd.arrow
/vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/struprompting/tasks/fewshot_task.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  return np.array(src_tokens), np.array(gpt_loss_mask), np.array(labels), max_len
2023-09-15 00:29:36 | WARNING | fairseq.models.fairseq_model | using 'args' is deprecated, please update your code to use dataclass config
2023-09-15 00:29:36 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): GPTDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 2048, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=2048, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=2048, bias=True)
        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=2048, out_features=51200, bias=False)
  )
)
2023-09-15 00:29:36 | INFO | fairseq_cli.train | task: FewshotEval
2023-09-15 00:29:36 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2023-09-15 00:29:36 | INFO | fairseq_cli.train | criterion: FewshotEvalCriterion
2023-09-15 00:29:36 | INFO | fairseq_cli.train | num. shared model params: 1,313,460,224 (num. trained: 2,048)
2023-09-15 00:29:36 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-15 00:29:36 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-09-15 00:29:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-09-15 00:29:36 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 2
2023-09-15 00:29:36 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt
2023-09-15 00:29:36 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt
2023-09-15 00:29:36 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-15 00:29:36 | INFO | fairseq_cli.train | Start validating
2023-09-15 00:29:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
------------ example 0 input str train is ['Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:', 'Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:']
------------ example 0 label str train is [' objective', ' subjective']
------------ example 1 input str train is ["Input: and if she 's lucky , she may find love along the way . Type:", "Input: and if she 's lucky , she may find love along the way . Type:"]
------------ example 1 label str train is [' objective', ' subjective']
------------ example 0 input str valid is ['Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:', 'Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:']
------------ example 0 label str valid is [' objective', ' subjective']
------------ example 1 input str valid is ['Input: if the story lacks bite , the performances are never less than affectionate . Type:', 'Input: if the story lacks bite , the performances are never less than affectionate . Type:']
------------ example 1 label str valid is [' objective', ' subjective']
NOTE: true K of baseline is 32, max valid len is 174
| Loaded valid with 4000 samples
| Loaded train with 4000 samples
2023-09-15 03:49:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.631 | nll_loss 0 | accuracy 90 | f1 0 | pos_proportion 51.7 | neg_proportion 48.3 | wps 384.2 | wpb 2299.7 | bsz 1 | num_updates 0
