2023-09-14 11:27:52 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-09-14 11:28:08 | INFO | fairseq_cli.train | {'task': {'_name': 'fs_eval', 'data': '-', 'seed': 4, 'eval_data': 'subj', 'test_split': 'test', 'required_batch_size_multiple': 1, 'gpt2_encoder_json': 'base_dir/gpt_icl/encoder.json', 'gpt2_vocab_bpe': 'base_dir/gpt_icl/vocab.bpe', 'gpt_dict': 'base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', 'ana_attn': 1, 'ana_rlt_dir': 'base_dir/ana_rlt/en_dense_lm_1_3b/subj', 'ana_setting': 'ft', 'optim_group': 'attn_kv', 'tokens_per_sample': 2048, 'max_target_positions': None, 'k': 32, 'temp_index': 0, 'permut_index': 0}, 'model': {'_name': 'gptmodel_large', 'activation_fn': 'gelu', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 2048, 'decoder_output_dim': 2048, 'decoder_input_dim': 2048, 'decoder_ffn_embed_dim': 8192, 'decoder_layers': 24, 'decoder_attention_heads': 32, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 2048, 'max_target_positions': None, 'tpu': False, 'gpt_model_path': 'base_dir/gpt_icl/en_dense_lm_1_3b/model.pt', 'use_linearization': '0', 'sum_extra_jvp_result': True}, 'criterion': {'_name': 'fs_ft', 'is_generation': False, 'beam': 3}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.003]}, 'common': {'no_progress_bar': False, 'log_interval': 1, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 256, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1000000, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 1000000, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'max_epoch': 1, 'max_update': 1000000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.003], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'save_dir': 'base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'beam': 3, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'buffer_size': 0, 'input': '-'}, 'ema': {'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'tokenizer': None, 'bpe': None, 'optimizer': Namespace(no_progress_bar=False, log_interval=1, log_format='simple', log_file=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=4, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=256, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', tokenizer=None, bpe=None, optimizer='sgd', lr_scheduler='fixed', criterion='fs_ft', task='fs_eval', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=1, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1000000, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=True, max_tokens_valid=None, batch_size_valid='1', max_valid_steps=None, curriculum=1000000, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=True, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='gptmodel_large', max_epoch=1, max_update=1000000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.003], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=1000000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='-', eval_data='subj', test_split='test', gpt2_encoder_json='base_dir/gpt_icl/encoder.json', gpt2_vocab_bpe='base_dir/gpt_icl/vocab.bpe', gpt_dict='base_dir/gpt_icl/en_dense_lm_1_3b/dict.txt', ana_attn=1, ana_rlt_dir='base_dir/ana_rlt/en_dense_lm_1_3b/subj', ana_setting='ft', optim_group='attn_kv', tokens_per_sample=2048, max_target_positions=None, k=32, temp_index=0, permut_index=0, momentum=0.0, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, is_generation=False, beam=3, checkpoint_activations=True, gpt_model_path='base_dir/gpt_icl/en_dense_lm_1_3b/model.pt', use_linearization='0', no_seed_provided=False, decoder_layers=24, decoder_embed_dim=2048, decoder_attention_heads=32, decoder_learned_pos=False, offload_activations=False, decoder_input_dim=2048, decoder_output_dim=2048, decoder_ffn_embed_dim=8192, dropout=0.0, attention_dropout=0.0, activation_fn='gelu', share_decoder_input_output_embed=True, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, decoder_layerdrop=0, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, base_layers=0, base_sublayers=1, base_shuffle=False, add_bos_token=False, no_token_positional_embeddings=False, character_embeddings=False, decoder_normalize_before=True, no_decoder_final_norm=False, adaptive_input=False, adaptive_input_factor=4, adaptive_input_cutoff=None, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=False, layernorm_embedding=False, scale_fc=False, scale_attn=False, scale_heads=False, scale_resids=False, _name='sgd')}
2023-09-14 11:28:08 | INFO | struprompting.tasks.fs_eval | dictionary: 51200 types
2023-09-14 11:28:10 | WARNING | datasets.builder | Using custom data configuration SetFit--subj-693a635c625bebac
2023-09-14 11:28:10 | WARNING | datasets.builder | Reusing dataset json (/a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  7.02it/s]100%|██████████| 2/2 [00:00<00:00, 10.85it/s]
2023-09-14 11:28:10 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-41679576bb6cfc7c.arrow
2023-09-14 11:28:10 | WARNING | datasets.arrow_dataset | Loading cached shuffled indices for dataset at /a/home/cc/students/cs/giladd/.cache/huggingface/datasets/SetFit___json/SetFit--subj-693a635c625bebac/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-6b58bb8b23415efd.arrow
/vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/struprompting/tasks/fewshot_task.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  return np.array(src_tokens), np.array(gpt_loss_mask), np.array(labels), max_len
2023-09-14 11:29:27 | WARNING | fairseq.models.fairseq_model | using 'args' is deprecated, please update your code to use dataclass config
2023-09-14 11:29:29 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): GPTDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 2048, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=2048, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=2048, bias=True)
        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=2048, out_features=51200, bias=False)
  )
)
2023-09-14 11:29:29 | INFO | fairseq_cli.train | task: FewshotEval
2023-09-14 11:29:29 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2023-09-14 11:29:29 | INFO | fairseq_cli.train | criterion: FewshotFTCriterion
2023-09-14 11:29:29 | INFO | fairseq_cli.train | num. shared model params: 1,313,460,224 (num. trained: 1,313,460,224)
2023-09-14 11:29:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-14 11:29:37 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2023-09-14 11:29:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-09-14 11:29:37 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-14 11:29:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-09-14 11:29:37 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-09-14 11:29:37 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 1
2023-09-14 11:29:37 | INFO | fairseq.trainer | Preparing to load checkpoint base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint_last.pt
2023-09-14 11:29:37 | INFO | fairseq.trainer | No existing checkpoint found base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint_last.pt
2023-09-14 11:29:37 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-14 11:29:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 32
2023-09-14 11:29:37 | INFO | fairseq.trainer | begin training epoch 1
2023-09-14 11:29:37 | INFO | fairseq_cli.train | Start fine-tuning
------------ example 0 input str train is ['Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:', 'Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:']
------------ example 0 label str train is [' objective', ' subjective']
------------ example 1 input str train is ["Input: and if she 's lucky , she may find love along the way . Type:", "Input: and if she 's lucky , she may find love along the way . Type:"]
------------ example 1 label str train is [' objective', ' subjective']
------------ example 0 input str valid is ['Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:', 'Input: never giving up the fight to win the war , mcnamara is silently planning , waiting for his moment to strike back at the enemy . Type:']
------------ example 0 label str valid is [' objective', ' subjective']
------------ example 1 input str valid is ['Input: if the story lacks bite , the performances are never less than affectionate . Type:', 'Input: if the story lacks bite , the performances are never less than affectionate . Type:']
------------ example 1 label str valid is [' objective', ' subjective']
NOTE: true K of baseline is 32, max valid len is 174
------------ example 0 input str train is ['Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:', 'Input: a spellbinding african film about the modern condition of rootlessness , a state experienced by millions around the globe . Type:']
------------ example 0 label str train is [' objective', ' subjective']
------------ example 1 input str train is ["Input: and if she 's lucky , she may find love along the way . Type:", "Input: and if she 's lucky , she may find love along the way . Type:"]
------------ example 1 label str train is [' objective', ' subjective']
NOTE: true K of baseline is 32
| Loaded valid with 4000 samples
| Loaded train with 32 samples
/vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 10546.9794921875
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7524.59814453125
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4540.81005859375
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3687.95654296875
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2806.513671875
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2089.436767578125
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1415.8533935546875
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1029.4986572265625
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 850.216064453125
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 722.7949829101562
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 641.5740966796875
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 566.9934692382812
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 465.8963928222656
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 413.6326904296875
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 401.623046875
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 436.97784423828125
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 433.2509765625
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 464.65423583984375
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 398.6169738769531
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 340.4513854980469
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 297.805908203125
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 305.49041748046875
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 305.53082275390625
2023-09-14 11:29:44 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 495.401611328125
2023-09-14 11:29:44 | INFO | train_inner | epoch 001:      1 / 32 loss=490.281, sample_size=28, ntokens=29, wps=0, ups=0, wpb=29, bsz=1, num_updates=1, lr=0.003, gnorm=38.702, clip=100, loss_scale=4, train_wall=7, gb_free=17.3, wall=8
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 6431.36767578125
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4728.95751953125
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2915.23388671875
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2337.146484375
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1791.6734619140625
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1198.77197265625
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 901.4109497070312
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 644.7183227539062
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 554.3994140625
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 534.5135498046875
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 466.8833312988281
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 467.4366455078125
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 383.4473571777344
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 350.0611572265625
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 353.6986389160156
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 378.98779296875
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 378.29315185546875
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 381.97027587890625
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 305.41119384765625
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 241.25286865234375
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 206.52239990234375
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 200.51258850097656
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 181.83059692382812
2023-09-14 11:29:46 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 404.8536071777344
2023-09-14 11:29:46 | INFO | train_inner | epoch 001:      2 / 32 loss=429.211, sample_size=20, ntokens=21, wps=16.4, ups=0.78, wpb=21, bsz=1, num_updates=2, lr=0.003, gnorm=36.721, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=9
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 10907.6904296875
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7880.06591796875
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4837.1904296875
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3732.097900390625
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2816.062744140625
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1956.211181640625
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1294.7994384765625
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 931.0894165039062
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 863.0650024414062
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 741.4359130859375
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 642.1650390625
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 604.4815673828125
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 548.03955078125
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 446.2078857421875
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 439.2360534667969
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 452.31036376953125
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 425.96600341796875
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 435.702392578125
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 350.2903747558594
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 272.39093017578125
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 240.12814331054688
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 222.05218505859375
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 216.05807495117188
2023-09-14 11:29:48 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 440.1761779785156
2023-09-14 11:29:48 | INFO | train_inner | epoch 001:      3 / 32 loss=440.78, sample_size=32, ntokens=33, wps=18, ups=0.54, wpb=33, bsz=1, num_updates=3, lr=0.003, gnorm=36.577, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=11
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 15764.2744140625
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 11777.4111328125
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7091.3505859375
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5073.00439453125
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3532.13623046875
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2758.70458984375
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1989.5672607421875
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1480.0938720703125
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1286.7008056640625
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1146.4378662109375
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 995.7059936523438
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 903.0717163085938
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 703.490234375
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 585.821044921875
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 575.6953735351562
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 649.4494018554688
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 707.563232421875
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 652.5441284179688
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 561.5482177734375
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 505.993408203125
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 432.5387268066406
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 400.367919921875
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 441.0133361816406
2023-09-14 11:29:50 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 579.5910034179688
2023-09-14 11:29:50 | INFO | train_inner | epoch 001:      4 / 32 loss=443.601, sample_size=48, ntokens=49, wps=17.5, ups=0.36, wpb=49, bsz=1, num_updates=4, lr=0.003, gnorm=34.875, clip=100, loss_scale=4, train_wall=3, gb_free=16.8, wall=14
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7055.49951171875
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5070.46826171875
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3258.5595703125
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2325.77734375
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1659.734619140625
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1326.11962890625
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 873.9011840820312
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 642.4691162109375
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 565.84814453125
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 502.30474853515625
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 412.4062194824219
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 391.7940673828125
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 339.12255859375
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 316.84783935546875
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 323.2414855957031
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 374.3917236328125
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 355.9129638671875
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 368.03875732421875
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 323.4905700683594
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 289.3757019042969
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 268.3421936035156
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 251.32186889648438
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 211.67041015625
2023-09-14 11:29:52 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 407.1244812011719
2023-09-14 11:29:52 | INFO | train_inner | epoch 001:      5 / 32 loss=468.453, sample_size=21, ntokens=22, wps=17.7, ups=0.81, wpb=22, bsz=1, num_updates=5, lr=0.003, gnorm=40.685, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=15
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5873.58251953125
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4335.00634765625
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2787.81982421875
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1880.593994140625
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1286.1121826171875
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1044.840087890625
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 728.9090576171875
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 527.4426879882812
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 520.832763671875
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 467.1037292480469
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 384.56341552734375
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 358.47283935546875
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 320.0263366699219
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 288.497802734375
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 298.9094543457031
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 307.0920715332031
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 324.21697998046875
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 348.0213928222656
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 268.23712158203125
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 220.73190307617188
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 195.9258270263672
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 207.50540161132812
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 246.4886474609375
2023-09-14 11:29:53 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 405.81585693359375
2023-09-14 11:29:53 | INFO | train_inner | epoch 001:      6 / 32 loss=464.979, sample_size=19, ntokens=20, wps=17, ups=0.85, wpb=20, bsz=1, num_updates=6, lr=0.003, gnorm=38.661, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=16
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 14446.0224609375
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 10552.333984375
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7278.92138671875
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4691.9091796875
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3226.380859375
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2771.42041015625
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1848.120361328125
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1327.5836181640625
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1147.962646484375
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1007.133544921875
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 753.4091186523438
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 642.8016357421875
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 507.37591552734375
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 435.70416259765625
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 471.91143798828125
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 527.614501953125
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 574.2960205078125
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 538.99169921875
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 462.7129211425781
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 425.9996337890625
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 357.9523010253906
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 365.4012451171875
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 355.85675048828125
2023-09-14 11:29:55 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 490.5583801269531
2023-09-14 11:29:55 | INFO | train_inner | epoch 001:      7 / 32 loss=461.193, sample_size=45, ntokens=46, wps=18.2, ups=0.39, wpb=46, bsz=1, num_updates=7, lr=0.003, gnorm=38.569, clip=100, loss_scale=4, train_wall=3, gb_free=16.9, wall=19
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 12777.2626953125
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 9116.9951171875
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 6596.60107421875
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4133.76611328125
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2847.390625
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2358.45947265625
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1513.292724609375
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1093.0592041015625
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 961.4530639648438
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 831.9537353515625
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 658.2677612304688
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 560.2321166992188
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 480.6735534667969
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 441.9836120605469
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 459.7059326171875
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 542.3639526367188
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 556.353759765625
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 535.1866455078125
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 450.8205261230469
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 367.8485107421875
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 321.7934265136719
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 330.0584716796875
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 337.36480712890625
2023-09-14 11:29:58 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 503.1634521484375
2023-09-14 11:29:58 | INFO | train_inner | epoch 001:      8 / 32 loss=420.713, sample_size=44, ntokens=45, wps=18.5, ups=0.41, wpb=45, bsz=1, num_updates=8, lr=0.003, gnorm=35.438, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=21
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 11561.05859375
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 8236.32421875
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 6355.45849609375
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3576.85498046875
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2504.744384765625
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2151.986083984375
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1329.4417724609375
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 931.1912231445312
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 777.8811645507812
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 671.5706787109375
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 527.6486206054688
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 476.3717041015625
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 413.00634765625
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 399.27264404296875
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 436.1119384765625
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 496.0251770019531
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 544.1172485351562
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 524.9434814453125
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 406.9806823730469
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 353.06488037109375
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 331.77532958984375
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 279.7994689941406
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 281.09783935546875
2023-09-14 11:30:00 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 466.6443786621094
2023-09-14 11:30:00 | INFO | train_inner | epoch 001:      9 / 32 loss=408.464, sample_size=39, ntokens=40, wps=19.1, ups=0.48, wpb=40, bsz=1, num_updates=9, lr=0.003, gnorm=37.561, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=23
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7433.62255859375
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5252.046875
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4495.66845703125
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2156.6416015625
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1471.307373046875
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1221.95361328125
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 739.9485473632812
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 562.0775756835938
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 518.5535888671875
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 483.08062744140625
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 424.8796081542969
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 401.5910949707031
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 331.9269714355469
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 329.27252197265625
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 355.480224609375
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 366.81365966796875
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 414.1122131347656
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 350.8681945800781
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 286.1697692871094
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 244.5149688720703
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 212.59588623046875
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 183.95411682128906
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 185.43450927734375
2023-09-14 11:30:02 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 378.09588623046875
2023-09-14 11:30:02 | INFO | train_inner | epoch 001:     10 / 32 loss=379.721, sample_size=28, ntokens=29, wps=16.8, ups=0.58, wpb=29, bsz=1, num_updates=10, lr=0.003, gnorm=36.353, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=25
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7838.3037109375
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5390.154296875
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4964.60693359375
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2133.295654296875
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1497.8646240234375
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1277.564697265625
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 764.9739990234375
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 559.8364868164062
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 527.73388671875
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 490.1114501953125
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 434.0412292480469
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 393.2084045410156
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 362.9802551269531
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 340.1191711425781
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 386.4931335449219
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 456.2047119140625
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 485.4424743652344
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 451.4191589355469
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 333.86187744140625
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 264.75714111328125
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 208.78045654296875
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 194.49159240722656
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 211.09011840820312
2023-09-14 11:30:03 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 396.549560546875
2023-09-14 11:30:03 | INFO | train_inner | epoch 001:     11 / 32 loss=394.965, sample_size=29, ntokens=30, wps=19.4, ups=0.65, wpb=30, bsz=1, num_updates=11, lr=0.003, gnorm=36.932, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=26
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 15864.4326171875
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 11268.4716796875
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 10403.2724609375
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3717.004638671875
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2307.92333984375
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1788.8389892578125
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1117.51513671875
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1044.2086181640625
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1162.27587890625
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1196.1749267578125
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 986.2799682617188
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 849.0967407226562
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 710.0133056640625
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 637.1990966796875
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 659.4076538085938
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 703.4606323242188
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 780.6864013671875
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 722.7604370117188
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 552.2880859375
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 504.0364074707031
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 390.5929870605469
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 319.45355224609375
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 312.5199890136719
2023-09-14 11:30:07 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 533.7914428710938
2023-09-14 11:30:07 | INFO | train_inner | epoch 001:     12 / 32 loss=389.148, sample_size=59, ntokens=60, wps=16.5, ups=0.27, wpb=60, bsz=1, num_updates=12, lr=0.003, gnorm=34.92, clip=100, loss_scale=4, train_wall=4, gb_free=16.8, wall=30
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 13570.94140625
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 8974.5078125
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 9104.0654296875
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3004.8623046875
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1892.5589599609375
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1371.344970703125
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 828.3298950195312
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 693.971923828125
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 758.4578247070312
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 856.106201171875
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 625.24169921875
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 577.6373901367188
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 454.2345886230469
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 390.1720275878906
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 373.8720397949219
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 433.902099609375
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 477.82080078125
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 492.775146484375
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 433.4621276855469
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 404.0461730957031
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 330.853759765625
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 310.65716552734375
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 348.8559875488281
2023-09-14 11:30:09 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 691.2066650390625
2023-09-14 11:30:09 | INFO | train_inner | epoch 001:     13 / 32 loss=414.893, sample_size=46, ntokens=47, wps=19, ups=0.4, wpb=47, bsz=1, num_updates=13, lr=0.003, gnorm=38.131, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=33
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 8203.1005859375
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5432.8994140625
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5918.19384765625
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1417.8553466796875
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 610.706787109375
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 362.1732177734375
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 365.9113464355469
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 443.0152893066406
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 547.0450439453125
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 619.816650390625
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 478.6976318359375
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 457.72271728515625
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 371.6026916503906
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 335.9736633300781
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 319.8359069824219
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 316.0421447753906
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 340.4501647949219
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 306.2870178222656
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 257.450439453125
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 231.11563110351562
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 273.41412353515625
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 382.9527587890625
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 519.2114868164062
2023-09-14 11:30:11 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 800.4915161132812
2023-09-14 11:30:11 | INFO | train_inner | epoch 001:     14 / 32 loss=424.445, sample_size=30, ntokens=31, wps=17.3, ups=0.56, wpb=31, bsz=1, num_updates=14, lr=0.003, gnorm=38.67, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=34
2023-09-14 11:30:12 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7656.30029296875
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4906.6416015625
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5368.86181640625
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1155.187255859375
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 487.5360107421875
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 310.84429931640625
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 307.2518005371094
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 356.4045715332031
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 432.58892822265625
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 465.09979248046875
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 384.5169982910156
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 360.3645324707031
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 290.7085266113281
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 263.1187744140625
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 253.541259765625
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 259.5361633300781
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 260.8636474609375
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 235.41270446777344
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 198.19308471679688
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 187.6556854248047
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 208.28273010253906
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 303.1576232910156
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 472.6141357421875
2023-09-14 11:30:13 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 908.171142578125
2023-09-14 11:30:13 | INFO | train_inner | epoch 001:     15 / 32 loss=418.367, sample_size=29, ntokens=30, wps=19.7, ups=0.66, wpb=30, bsz=1, num_updates=15, lr=0.003, gnorm=35.755, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=36
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3996.9755859375
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2535.54443359375
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2507.3984375
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 407.7276611328125
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 182.02284240722656
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 167.36822509765625
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 213.39964294433594
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 257.5928955078125
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 321.40350341796875
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 348.781982421875
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 304.4980773925781
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 303.0930480957031
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 250.6555938720703
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 223.9245147705078
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 217.6891632080078
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 222.87933349609375
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 238.17935180664062
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 235.3764190673828
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 187.60411071777344
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 168.5828399658203
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 185.6170654296875
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 249.73464965820312
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 335.7154541015625
2023-09-14 11:30:14 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 446.4177551269531
2023-09-14 11:30:14 | INFO | train_inner | epoch 001:     16 / 32 loss=364.155, sample_size=20, ntokens=21, wps=18.3, ups=0.87, wpb=21, bsz=1, num_updates=16, lr=0.003, gnorm=28.651, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=37
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 6987.662109375
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4659.73681640625
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3751.03125
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 510.26776123046875
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 317.05084228515625
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 279.690673828125
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 331.11285400390625
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 381.24273681640625
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 459.97235107421875
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 478.8990173339844
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 420.6036682128906
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 399.7126770019531
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 334.7411193847656
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 302.526123046875
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 299.2137451171875
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 311.3480529785156
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 350.5521240234375
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 341.4593811035156
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 275.6877746582031
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 276.6097106933594
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 294.5311584472656
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 358.6416015625
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 495.24688720703125
2023-09-14 11:30:15 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 727.6437377929688
2023-09-14 11:30:15 | INFO | train_inner | epoch 001:     17 / 32 loss=393.113, sample_size=31, ntokens=32, wps=19.4, ups=0.61, wpb=32, bsz=1, num_updates=17, lr=0.003, gnorm=28.384, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=39
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7532.16015625
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4999.796875
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4163.8408203125
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 592.9132080078125
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 319.9114990234375
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 284.72149658203125
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 313.6767883300781
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 352.4031677246094
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 404.0233154296875
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 427.6375732421875
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 373.2227783203125
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 354.7525939941406
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 291.6356201171875
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 266.73260498046875
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 264.9244079589844
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 274.0674133300781
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 294.5855712890625
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 282.01873779296875
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 223.87100219726562
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 217.29913330078125
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 235.47103881835938
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 284.5367431640625
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 417.388671875
2023-09-14 11:30:17 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 767.6810302734375
2023-09-14 11:30:17 | INFO | train_inner | epoch 001:     18 / 32 loss=417.615, sample_size=31, ntokens=32, wps=19.8, ups=0.62, wpb=32, bsz=1, num_updates=18, lr=0.003, gnorm=29.949, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=40
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 14665.6455078125
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 10591.61328125
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 6175.90869140625
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 966.6853637695312
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 611.1375122070312
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 537.3379516601562
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 579.9039306640625
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 634.718017578125
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 740.07470703125
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 779.1353149414062
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 687.1639404296875
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 648.3445434570312
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 536.2781372070312
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 484.90460205078125
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 475.4566955566406
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 503.61962890625
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 555.9948120117188
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 558.48095703125
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 433.0984802246094
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 431.10614013671875
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 454.1595458984375
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 549.3956909179688
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 783.09228515625
2023-09-14 11:30:20 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1366.1075439453125
2023-09-14 11:30:20 | INFO | train_inner | epoch 001:     19 / 32 loss=413.321, sample_size=60, ntokens=61, wps=20.4, ups=0.34, wpb=61, bsz=1, num_updates=19, lr=0.003, gnorm=28.076, clip=100, loss_scale=4, train_wall=3, gb_free=16.8, wall=43
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 10934.65625
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7842.375
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5565.23095703125
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 686.0327758789062
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 355.0452575683594
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 332.2381591796875
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 365.2966003417969
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 403.4251708984375
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 454.1974792480469
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 471.529541015625
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 405.4240417480469
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 379.117919921875
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 314.9517822265625
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 291.23785400390625
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 283.5660095214844
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 295.2027587890625
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 294.1023864746094
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 289.2253723144531
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 237.81719970703125
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 225.355712890625
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 241.07691955566406
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 295.6958923339844
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 428.64483642578125
2023-09-14 11:30:22 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1176.4879150390625
2023-09-14 11:30:22 | INFO | train_inner | epoch 001:     20 / 32 loss=429.215, sample_size=41, ntokens=42, wps=19.4, ups=0.46, wpb=42, bsz=1, num_updates=20, lr=0.003, gnorm=31.909, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=45
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 10500.798828125
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 7939.841796875
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3825.84619140625
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 636.56103515625
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 411.6593017578125
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 374.0516052246094
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 416.9722595214844
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 457.72467041015625
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 517.4066772460938
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 536.1373901367188
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 474.015380859375
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 440.96722412109375
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 366.1104736328125
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 333.0595397949219
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 325.4105224609375
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 342.588623046875
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 364.2971496582031
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 358.36102294921875
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 283.6521911621094
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 279.8528747558594
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 298.2002868652344
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 362.43450927734375
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 644.0409545898438
2023-09-14 11:30:25 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 986.089111328125
2023-09-14 11:30:25 | INFO | train_inner | epoch 001:     21 / 32 loss=406.044, sample_size=46, ntokens=47, wps=19.1, ups=0.41, wpb=47, bsz=1, num_updates=21, lr=0.003, gnorm=26.329, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=48
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3807.418212890625
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2472.4560546875
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 717.3245239257812
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 242.36068725585938
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 165.59588623046875
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 148.1846160888672
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 173.58531188964844
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 194.68411254882812
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 225.48448181152344
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 228.36264038085938
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 208.3910369873047
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 201.12632751464844
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 167.10687255859375
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 151.6965789794922
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 153.14877319335938
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 162.47613525390625
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 180.08888244628906
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 179.18553161621094
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 145.59181213378906
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 146.51353454589844
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 148.29627990722656
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 181.29559326171875
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 235.35894775390625
2023-09-14 11:30:26 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 604.9471435546875
2023-09-14 11:30:26 | INFO | train_inner | epoch 001:     22 / 32 loss=370.651, sample_size=21, ntokens=22, wps=18.3, ups=0.83, wpb=22, bsz=1, num_updates=22, lr=0.003, gnorm=21.515, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=49
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 14609.720703125
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 12083.384765625
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2490.896728515625
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 874.5364990234375
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 605.38525390625
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 547.772705078125
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 603.52099609375
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 658.9669189453125
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 756.0821533203125
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 765.3740844726562
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 695.1517333984375
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 655.0221557617188
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 545.8939208984375
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 489.13531494140625
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 484.10955810546875
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 505.3806457519531
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 548.7147216796875
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 560.2067260742188
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 439.50750732421875
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 431.8522644042969
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 455.21417236328125
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 536.609375
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 648.7094116210938
2023-09-14 11:30:29 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1639.9046630859375
2023-09-14 11:30:29 | INFO | train_inner | epoch 001:     23 / 32 loss=398.682, sample_size=68, ntokens=69, wps=20, ups=0.29, wpb=69, bsz=1, num_updates=23, lr=0.003, gnorm=24.691, clip=100, loss_scale=4, train_wall=3, gb_free=16.7, wall=53
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3215.913330078125
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2182.786376953125
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 601.55419921875
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 209.98812866210938
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 145.31356811523438
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 132.25904846191406
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 148.61813354492188
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 169.7574462890625
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 198.38739013671875
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 197.26451110839844
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 180.0114288330078
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 172.13198852539062
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 146.57313537597656
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 131.22576904296875
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 132.32028198242188
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 141.87518310546875
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 146.80593872070312
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 145.8542938232422
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 116.98574829101562
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 117.09784698486328
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 119.54458618164062
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 144.15118408203125
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 176.51158142089844
2023-09-14 11:30:31 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 508.1377258300781
2023-09-14 11:30:31 | INFO | train_inner | epoch 001:     24 / 32 loss=369.937, sample_size=18, ntokens=19, wps=14.1, ups=0.74, wpb=19, bsz=1, num_updates=24, lr=0.003, gnorm=20.931, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=54
2023-09-14 11:30:32 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 6345.59521484375
2023-09-14 11:30:32 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4874.14013671875
2023-09-14 11:30:32 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1333.5159912109375
2023-09-14 11:30:32 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 347.725830078125
2023-09-14 11:30:32 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 225.8348846435547
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 202.08677673339844
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 230.53451538085938
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 250.9744110107422
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 290.2306213378906
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 292.2491455078125
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 257.50897216796875
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 239.23680114746094
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 198.44998168945312
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 181.08493041992188
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 179.40371704101562
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 188.55848693847656
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 200.93997192382812
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 205.2168426513672
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 163.45899963378906
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 156.46469116210938
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 163.13865661621094
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 196.0918426513672
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 211.17581176757812
2023-09-14 11:30:33 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 984.2574462890625
2023-09-14 11:30:33 | INFO | train_inner | epoch 001:     25 / 32 loss=396.007, sample_size=32, ntokens=33, wps=16.8, ups=0.51, wpb=33, bsz=1, num_updates=25, lr=0.003, gnorm=22.454, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=56
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4668.90771484375
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3539.03515625
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 716.8687744140625
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 306.50244140625
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 218.25704956054688
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 196.93789672851562
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 218.1820526123047
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 240.47598266601562
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 280.2630920410156
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 284.8261413574219
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 256.1308898925781
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 242.7263641357422
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 201.80288696289062
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 181.1907196044922
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 180.3065185546875
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 192.42568969726562
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 210.70635986328125
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 211.79237365722656
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 170.4336395263672
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 171.68948364257812
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 176.12564086914062
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 212.61676025390625
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 312.69482421875
2023-09-14 11:30:34 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 674.32421875
2023-09-14 11:30:34 | INFO | train_inner | epoch 001:     26 / 32 loss=374.293, sample_size=26, ntokens=27, wps=19.3, ups=0.71, wpb=27, bsz=1, num_updates=26, lr=0.003, gnorm=20.727, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=57
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4689.4462890625
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3515.873046875
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 556.2415161132812
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 247.07418823242188
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 167.7867889404297
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 157.24295043945312
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 171.8241729736328
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 188.6245574951172
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 218.74571228027344
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 217.4664306640625
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 192.2608642578125
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 182.67715454101562
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 154.4986114501953
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 138.71115112304688
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 138.0086669921875
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 147.25128173828125
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 158.30682373046875
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 155.5316162109375
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 130.399169921875
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 122.2744369506836
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 125.1753158569336
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 148.74978637695312
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 166.7517852783203
2023-09-14 11:30:35 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 910.0999755859375
2023-09-14 11:30:35 | INFO | train_inner | epoch 001:     27 / 32 loss=393.453, sample_size=25, ntokens=26, wps=19.4, ups=0.74, wpb=26, bsz=1, num_updates=27, lr=0.003, gnorm=21.04, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=59
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4692.1748046875
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3560.1357421875
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 582.85791015625
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 278.2665710449219
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 195.57276916503906
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 178.51275634765625
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 204.90374755859375
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 224.5565185546875
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 260.7284851074219
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 259.0527648925781
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 233.0262451171875
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 217.39932250976562
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 180.71153259277344
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 163.96835327148438
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 166.29052734375
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 173.71658325195312
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 188.52938842773438
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 189.1207733154297
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 149.7889862060547
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 147.3734588623047
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 151.16769409179688
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 181.1813201904297
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 208.72271728515625
2023-09-14 11:30:37 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 765.4111938476562
2023-09-14 11:30:37 | INFO | train_inner | epoch 001:     28 / 32 loss=376.725, sample_size=27, ntokens=28, wps=19.8, ups=0.71, wpb=28, bsz=1, num_updates=28, lr=0.003, gnorm=19.608, clip=100, loss_scale=4, train_wall=1, gb_free=16.9, wall=60
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 6777.40625
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5648.54833984375
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1238.3924560546875
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 412.13812255859375
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 303.3668212890625
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 275.3103942871094
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 306.1375732421875
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 332.6200866699219
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 384.54766845703125
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 385.37481689453125
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 344.34942626953125
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 323.2196960449219
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 267.1578674316406
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 240.97425842285156
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 236.23048400878906
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 252.74537658691406
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 269.1858825683594
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 271.1285705566406
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 221.1890869140625
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 212.32838439941406
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 217.77345275878906
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 259.7847900390625
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 260.2534484863281
2023-09-14 11:30:39 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 955.4331665039062
2023-09-14 11:30:39 | INFO | train_inner | epoch 001:     29 / 32 loss=374.036, sample_size=38, ntokens=39, wps=20, ups=0.51, wpb=39, bsz=1, num_updates=29, lr=0.003, gnorm=20.324, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=62
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 6078.26171875
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 5045.5791015625
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 1075.5191650390625
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 339.90155029296875
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 232.1038360595703
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 215.47885131835938
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 238.19400024414062
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 258.29547119140625
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 291.3771667480469
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 292.02178955078125
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 260.8814697265625
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 244.8033905029297
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 200.7373504638672
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 182.5355987548828
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 179.86537170410156
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 187.2388458251953
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 200.71658325195312
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 200.22787475585938
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 166.37860107421875
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 156.99591064453125
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 161.509033203125
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 183.200439453125
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 196.4009552001953
2023-09-14 11:30:40 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 989.7958984375
2023-09-14 11:30:40 | INFO | train_inner | epoch 001:     30 / 32 loss=391.368, sample_size=32, ntokens=33, wps=19.9, ups=0.6, wpb=33, bsz=1, num_updates=30, lr=0.003, gnorm=22.157, clip=100, loss_scale=4, train_wall=2, gb_free=16.9, wall=64
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 4278.681640625
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3530.978271484375
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 728.8856811523438
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 259.4770202636719
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 183.96173095703125
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 173.5946807861328
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 193.95590209960938
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 211.3985595703125
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 239.71681213378906
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 243.1130828857422
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 216.6449432373047
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 200.5322265625
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 167.26541137695312
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 151.92974853515625
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 150.4444580078125
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 157.23377990722656
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 162.0054931640625
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 161.9787139892578
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 131.55886840820312
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 126.5461196899414
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 130.0097198486328
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 150.82705688476562
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 162.43502807617188
2023-09-14 11:30:42 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 762.5165405273438
2023-09-14 11:30:42 | INFO | train_inner | epoch 001:     31 / 32 loss=380.246, sample_size=25, ntokens=26, wps=19.9, ups=0.76, wpb=26, bsz=1, num_updates=31, lr=0.003, gnorm=20.148, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=65
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 3288.207763671875
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 2672.25146484375
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 394.08917236328125
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 255.5728302001953
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 181.9224090576172
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 162.70089721679688
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 183.71665954589844
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 205.82542419433594
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 243.56605529785156
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 240.96780395507812
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 220.0579376220703
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 210.8411865234375
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 172.9512939453125
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 154.8912811279297
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 155.84327697753906
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 164.25357055664062
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 178.52981567382812
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 179.0448760986328
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 144.79417419433594
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 138.5948486328125
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 141.40545654296875
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 168.16754150390625
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 173.5421142578125
2023-09-14 11:30:43 | INFO | fairseq.trainer | layer TransformerDecoderLayerBase(
  (dropout_module): FairseqDropout()
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_dropout_module): FairseqDropout()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
) total_norm 631.7230224609375
2023-09-14 11:30:43 | INFO | train_inner | epoch 001:     32 / 32 loss=353.541, sample_size=25, ntokens=26, wps=18.9, ups=0.73, wpb=26, bsz=1, num_updates=32, lr=0.003, gnorm=16.032, clip=100, loss_scale=4, train_wall=1, gb_free=17, wall=66
2023-09-14 11:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 32 updates
2023-09-14 11:30:43 | INFO | fairseq.trainer | Saving checkpoint to /vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint1.pt
2023-09-14 11:31:46 | INFO | fairseq.trainer | Finished saving checkpoint to /vol/joberant_nobck/data/NLP_368307701_2223/giladd/ft-vs-icl/base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint1.pt
2023-09-14 11:34:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint base_dir/ft_gpt/subj/en_dense_lm_1_3b/0.003/checkpoint1.pt (epoch 1 @ 32 updates, score None) (writing took 229.4241499332711 seconds)
2023-09-14 11:34:32 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-09-14 11:34:32 | INFO | train | epoch 001 | loss 409.152 | sample_size 33.844 | ntokens 34.844 | wps 3.8 | ups 0.11 | wpb 34.8 | bsz 1 | num_updates 32 | lr 0.003 | gnorm 30.046 | clip 100 | loss_scale 4 | train_wall 66 | gb_free 17 | wall 296
2023-09-14 11:34:32 | INFO | fairseq_cli.train | done training in 295.5 seconds
