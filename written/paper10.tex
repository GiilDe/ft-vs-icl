\documentclass[runningheads]{llncs}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{url}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{bbm}
\usepackage[capitalise]{cleveref}
\usepackage{arydshln}
\usepackage{wrapfig}

\usepackage{xcolor}
 \renewcommand\UrlFont{\color{blue}\rmfamily}



\begin{document}
%
\title{ICL-VS-FT} 
%\thanks{
%This research was supported by the Ministry of Science \& Technology, Israel.}} 

\author{Tomer Bar Natan\inst{1}}

\authorrunning{T.  Bar Natan et al.}
\titlerunning{ICL-VS-FT}

\institute{
Tel-Aviv University, Tel-Aviv, Israel \\
\email{\{tomerb5@mail,
hayit@eng\}.tau.ac.il}}
\maketitle      

\begin{abstract}
  Large pretrained language models have shown
  surprising in-context learning (ICL)
\keywords{NLP \and LLM \and   ICL \and Linearization }
\end{abstract}
\section{Introduction}

In recent years, large pretrained language models,
especially in Transformer-based architectures \cite{lipton2018}
 \bibliographystyle{splncs04}
 \bibliography{refs}




\end{document}


