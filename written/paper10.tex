\documentclass[runningheads]{llncs}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{url}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{bbm}
\usepackage[capitalise]{cleveref}
\usepackage{arydshln}
\usepackage{wrapfig}

\usepackage{xcolor}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\newcommand\sizeof[1]{\left|#1\right|}


\begin{document}
%
\title{ICL-VS-FT} 
%\thanks{
%This research was supported by the Ministry of Science \& Technology, Israel.}} 

\author{Tomer Bar Natan\inst{1}}

\authorrunning{T.  Bar Natan et al.}
\titlerunning{ICL-VS-FT}

\institute{
Tel-Aviv University, Tel-Aviv, Israel \\
\email{\{tomerb5@mail\}.tau.ac.il}}
\maketitle      

\begin{abstract}
  Large pretrained language models have shown
  surprising in-context learning (ICL)
\keywords{NLP \and LLM \and   ICL \and Linearization }
\end{abstract}
\section{Literature Review}
In-context learning (ICL) is a machine learning approach where a model fine-tunes its knowledge and adapts its behavior based on specific contextual information or examples, allowing it to perform better on tasks related to that context.
It enables models to leverage domain-specific or task-specific knowledge without extensive retraining, making them more versatile and adaptable.
In their work, \cite{NEURIPS2020_1457c0d6} explores the remarkable ability of language models, particularly GPT-3, to learn and perform tasks with minimal examples, demonstrating their potential as versatile few-shot learners.
The authors showcase the models' impressive performance across a wide range of tasks and emphasize their capacity to generalize from limited data, highlighting the transformative impact of these models on various natural language processing applications.

In recent research, there has been a growing interest in understanding the relationship between two key concepts: in-context learning (ICL) and gradient descent (GD)-based fine-tuning, particularly in the context of transformer models (\cite{pmlr-v202-von-oswald23a,2022arXiv221210559D}).
This research seeks to uncover how ICL, which involves adapting and learning in specific contexts, can be effectively integrated with the iterative optimization process of GD, especially when fine-tuning transformer models.
However, the majority of the examination was on models that had relaxed constraints and featured linear attention mechanisms:
\begin{equation}
  LinearAttn(K,V,q)=KV^q
\end{equation}

The paper \cite{pmlr-v202-von-oswald23a}, develops an explicit weight values for a linear self-attention layer, achieving an update equivalent to a single iteration of gradient descent (GD) aimed at minimizing mean squared error. Moreover, the authors demonstrate how multiple self-attention layers can progressively execute curvature adjustments, leading to enhancements over standard gradient descent.
They proposed the following:

Given a 1-head linear attention layer and
the tokens $e_{j} = (x_{j},y_{j})$, for $j = 1, . . . , N$, one can construct key, query and value matrices $W_{K}$, $W_{Q}$, $W_{V}$ as well
as the projection matrix P such that a Transformer step on 
every token $e_j$ is identical to the gradient-induced dynamics $e_j \rightarrow (x_j , y_j ) + (0, - \delta W x_j ) = (x_j , y_j ) + PVK^{T}q_j$
such that $e_j = (x_j , y_j - \delta y_j )$. For the test data token
$(x_{N+1}, y_{N+1})$ the dynamics are identical.

By doing so, they demonstrate the capability of linear attention to execute gradient descent on the deep representations constructed by the transformer.


Another paper (\cite{2022arXiv221210559D}) expand the findings from linear attention to conventional attention mechanisms, substantiating their claims with empirical data.
Inspired by \cite{Aizerman2019TheoreticalFO} and \cite{unknown}, the idea in this is paper to explain language models as meta-optimizers.

Consider $W_0$ and $\Delta W$, both belonging to $\mathbb{R}^{d_{out} \times d_{in}}$, where $W_0$ represents the initial parameter matrix, and $\Delta W$ signifies the updating matrix. Additionally, let $x$ be a member of $\mathbb{R}^{d_{in}}$, serving as the input representation. A linear layer, subject to optimization via gradient descent, can be articulated as follows:
\begin{equation}
  \mathcal{F}(x) = (W_0 + \Delta W)x \label{eq:2}
\end{equation}
In the context of the back-propagation algorithm, the determination of $\Delta W$ entails the aggregation of outer products derived from historical input representations $x'_i \in \mathbb{R}^{d_{in}}$ and their corresponding error signals $e_i \in \mathbb{R}^{d_{out}}$:
\begin{equation}
  \Delta W = \sum_{i} e_i \otimes x'_i \label{eq:3}
\end{equation}
Notably, $e_i$ is the result of scaling historical output gradients by $-\gamma$, the negative learning rate.

By equations \eqref{eq:2} and \eqref{eq:3}, we can derive the dual manifestation of linear layers, optimized through gradient descent, as follows:
\begin{align}
  \begin{split}
    \mathcal{F}(x) &= (W_0 + \Delta W)x \\
    &= W_0x + \Delta Wx \\
    &= W_0x + \sum_{i} (e_i \otimes x'_i)x \\
    &= W_0x + \sum_{i} e_i(x^{'T}_ix) \\
    &= W_0x + \text{LinearAttn}(E, X', x)
  \end{split}
  \label{eq:4}
\end{align}
Here, $E$ denotes historical output error signal values, $X'$ corresponds to historical inputs employed as keys, and $x$ serves as the current input, operating as the query.

Their experiments convincingly reveal that a model fine-tuned through gradient steps and a model prompted with in-context examples appear to perform analogous functions, exhibiting similar behaviors on inputs. 
Additionally, they observe significant similarities in the internal behaviors of these two models.


\section{Background and Preliminaries}
\input{background.tex}

\section{Method}
\input{method.tex}

 \bibliographystyle{splncs04}
 \bibliography{refs}




\end{document}


